{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Preprocessing\n",
    "\n",
    "This notebooks preprocesses functional MRI images. It requires that the anatomical preprocessing workflwo was already executed. This functional preprocessing workflow runs the following processing steps:\n",
    "\n",
    "1. Reorient Images to RAS\n",
    "1. Removal of non-steady state volumes \n",
    "1. Motion Correction with SPM\n",
    "1. Slice-wise Correction with SPM\n",
    "1. Brain Extraction with SPM and FSL\n",
    "1. High-Pass Filter with FSL\n",
    "1. Two- step coregistration using BBR with FSL, using WM segmentation from SPM\n",
    "1. Low-pass and band-pass smoothing with Nilearn\n",
    "\n",
    "Additional, this workflow also performs:\n",
    " - ACompCor\n",
    " - TCompCor\n",
    " - Artifact Detection\n",
    " - Computes Friston's 24-paramter model for motion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure Requirements\n",
    "\n",
    "The data structure to run this notebook should be according to the BIDS format. Note that the data should be in a session subfolder:\n",
    "\n",
    "    dataset\n",
    "    ├── analysis-func_task-{task_id}_specs.json\n",
    "    ├── sub-{sub_id}\n",
    "    │   └── ses-{sess_id}\n",
    "    │       └── func\n",
    "    │           ├── sub-{sub_id}_ses-{sess_id}_task-{task_id}_run-{run_id}_bold.nii.gz\n",
    "    └── task-{task_id}_bold.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Specifications\n",
    "\n",
    "This notebook will extract the relevant processing specifications from the `analysis-func_task-{task_id}_specs.json` file in the dataset folder. In the current setup, they are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of task to process\n",
    "task_id = 'fingerfootlips'\n",
    "\n",
    "import json\n",
    "from os.path import join as opj\n",
    "\n",
    "spec_file = opj('/data', 'analysis-func_task-%s_specs.json' % task_id)\n",
    "\n",
    "with open(spec_file) as f:\n",
    "    specs = json.load(f)\n",
    "\n",
    "specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to change any of those values manually, overwrite them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject names\n",
    "subject_list = specs['subject_list']\n",
    "\n",
    "# List of session names\n",
    "session_list = specs['session_list']\n",
    "\n",
    "# List of run names\n",
    "run_list = specs['run_list']\n",
    "\n",
    "# Width of smoothing kernels to apply\n",
    "fwhm = specs['fwhm']\n",
    "\n",
    "# Width of band-pass smoothing kernel to apply\n",
    "fwhm_bp = specs['fwhm_bp']\n",
    "\n",
    "# Requested isometric voxel resolution after coregistration\n",
    "voxel_res = specs['voxel_res']\n",
    "\n",
    "# Reference Slice or time point (in ms) required for slice wise correction\n",
    "ref_slice = specs['ref_slice']\n",
    "\n",
    "# High pass filter [in seconds] to apply\n",
    "hpf = specs['hpf']\n",
    "\n",
    "# Number of components to extract with ACompCor and TCompCor\n",
    "ncomp = specs['ncomp']\n",
    "\n",
    "# Number of cores to use\n",
    "n_proc = specs['n_proc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cerating the Workflow\n",
    "\n",
    "To ensure a good overview of the functional preprocessing, the workflow was divided into three subworkflows:\n",
    "\n",
    "1. The Main Workflow, i.e. doing the actual preprocessing\n",
    "2. The Confound Workflow, i.e. computing confound variables\n",
    "3. Visualization Workflow, i.e. visualizating relevant steps for quality control\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os.path import join as opj\n",
    "from nipype import Workflow, Node, IdentityInterface, Function\n",
    "from nipype.interfaces.image import Reorient\n",
    "from nipype.interfaces.spm import SliceTiming, Realign, Smooth\n",
    "from nipype.interfaces.fsl import FLIRT, MeanImage, BET, TemporalFilter, BinaryMaths, ExtractROI\n",
    "from nipype.interfaces.io import SelectFiles, DataSink\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "from nipype.algorithms.rapidart import ArtifactDetect\n",
    "from nipype.algorithms.confounds import ACompCor, TCompCor, NonSteadyStateDetector\n",
    "\n",
    "# Specify SPM location\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('/opt/spm12-dev/spm12_mcr/spm/spm12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Execution Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant folder paths and names\n",
    "exp_dir = '/output'\n",
    "out_dir = 'datasink'\n",
    "work_dir = 'workingdir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the Main Workflow\n",
    "\n",
    "### Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorient anatomical images to RAS\n",
    "reorient = Node(Reorient(orientation='RAS'), name='reorient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection of Non-Steady State volumes\n",
    "nonsteady_detection = Node(NonSteadyStateDetector(), name='nonsteady_detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of Non-Steady State volumes\n",
    "nonsteady_removal = Node(ExtractROI(output_type='NIFTI',\n",
    "                                    t_size=-1),\n",
    "                         name='nonsteady_removal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequence specifications of functional images\n",
    "def get_parameters(task_id):\n",
    "    \n",
    "    import json\n",
    "    import numpy as np\n",
    "    from os.path import join as opj\n",
    "        \n",
    "    func_desc = opj('/data', 'task-%s_bold.json' % task_id)\n",
    "\n",
    "    with open(func_desc) as f:\n",
    "        func_desc = json.load(f)\n",
    "\n",
    "    # Read out relevant parameters\n",
    "    TR = func_desc['RepetitionTime']\n",
    "    slice_order = func_desc['SliceTiming']\n",
    "    nslices = len(slice_order)\n",
    "    time_acquisition = float(TR)-(TR/nslices)\n",
    "    \n",
    "    return TR, slice_order, nslices, time_acquisition\n",
    "\n",
    "getParam = Node(Function(input_names=['task_id'],\n",
    "                         output_names=['TR', 'slice_order',\n",
    "                                       'nslices', 'time_acquisition'],\n",
    "                         function=get_parameters),\n",
    "                name='getParam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct for motion\n",
    "realign = Node(Realign(register_to_mean=True), name='realign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct for slice-wise acquisition\n",
    "slicetime = Node(SliceTiming(ref_slice=ref_slice), name='slicetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset TR value after SPM's slice time correction\n",
    "def add_TR_to_file(in_file, TR):\n",
    "    \n",
    "    import nibabel as nb\n",
    "    \n",
    "    # Load image\n",
    "    img = nb.load(in_file)\n",
    "    \n",
    "    # Reset TR\n",
    "    img.header.set_zooms(list(img.header.get_zooms()[:3]) + [TR])\n",
    "    \n",
    "    # Save file\n",
    "    out_file = in_file.replace('.nii', '_TR.nii')\n",
    "    img.to_filename(out_file)\n",
    "    del img\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "reset_TR = Node(Function(input_names=['in_file', 'TR'],\n",
    "                         output_names=['out_file'],\n",
    "                         function=add_TR_to_file),\n",
    "                name='reset_TR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove skull signal from functional images\n",
    "bet_func = Node(BET(functional=True,\n",
    "                    mask=True,\n",
    "                    output_type='NIFTI_GZ'),\n",
    "                name='bet_func')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply high-pass filter to data\n",
    "hp_filter = Node(TemporalFilter(output_type='NIFTI_GZ'),\n",
    "                 name='hp_filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sigma for temporal high-pass filter\n",
    "def get_sigma(TR, hpf):\n",
    "    return float(hpf)/float(TR)/2.\n",
    "\n",
    "hpf_sigma = Node(Function(input_names=['TR', 'hpf'],\n",
    "                          output_names=['sigma'],\n",
    "                          function=get_sigma),\n",
    "                 name='hpf_sigma')\n",
    "hpf_sigma.inputs.hpf = hpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute mean image before high-pass filter\n",
    "hp_mean = Node(MeanImage(dimension='T',\n",
    "                         output_type='NIFTI_GZ'),\n",
    "                  name='hp_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add  mean to high-passed filtered signal\n",
    "hp_combine = Node(BinaryMaths(operation='add',\n",
    "                              output_type='NIFTI_GZ'),\n",
    "                  name='hp_combine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-alignment of functional images to anatomical image\n",
    "coreg_pre = Node(FLIRT(dof=6,\n",
    "                       output_type='NIFTI_GZ'),\n",
    "                 name='coreg_pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coregistration of functional images to anatomical image with BBR\n",
    "# using WM segmentation\n",
    "coreg_bbr = Node(FLIRT(dof=6,\n",
    "                       cost='bbr',\n",
    "                       schedule=opj(os.getenv('FSLDIR'),\n",
    "                                    'etc/flirtsch/bbr.sch'),\n",
    "                       output_type='NIFTI_GZ'),\n",
    "                 name='coreg_bbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply coregistration warp to functional images\n",
    "applycoreg = Node(FLIRT(interp='spline',\n",
    "                        apply_isoxfm=voxel_res,\n",
    "                        datatype='short',\n",
    "                        output_type='NIFTI_GZ'),\n",
    "                 name='applycoreg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop coregistered files to reduce file size\n",
    "def crop_img(in_file):\n",
    "    \n",
    "    from nilearn.image import crop_img\n",
    "\n",
    "    # Crop image\n",
    "    out_file = in_file.replace('.nii', '_crop.nii')\n",
    "    crop_img(in_file).to_filename(out_file)\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "cropper = Node(Function(input_names=['in_file'],\n",
    "                        output_names=['out_file'],\n",
    "                        function=crop_img),\n",
    "               name='cropper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies gaussian spatial filter as in Sengupta, Pollmann & Hanke, 2018\n",
    "def gaussian_spatial_filter(in_file, fwhm, ftype='LP', bandwidth=1):\n",
    "\n",
    "    from nilearn.image import smooth_img\n",
    "    from nibabel.nifti1 import Nifti1Image\n",
    "\n",
    "    if ftype == 'LP':\n",
    "        img = smooth_img(in_file, fwhm=fwhm)\n",
    "        \n",
    "    elif ftype == 'BP':\n",
    "        LPF_bold_1 = smooth_img(in_file, fwhm=fwhm)\n",
    "        LPF_bold_2 = smooth_img(in_file, fwhm=fwhm - bandwidth)\n",
    "        BPF_bold = LPF_bold_2.get_fdata() - LPF_bold_1.get_fdata()\n",
    "        img = Nifti1Image(BPF_bold, LPF_bold_1.affine, LPF_bold_1.header)\n",
    "\n",
    "    # Save and return output file\n",
    "    out_file = in_file.replace('.nii', '_%s_%smm.nii' % (ftype, fwhm))\n",
    "    img.to_filename(out_file)\n",
    "    del img\n",
    "\n",
    "    return out_file\n",
    "\n",
    "# Spatial Band-Pass Filter\n",
    "smooth_lowpass = Node(Function(input_names=['in_file', 'fwhm', 'ftype'],\n",
    "                                  output_names=['out_file'],\n",
    "                                  function=gaussian_spatial_filter),\n",
    "                         name='smooth_lowpass')\n",
    "smooth_lowpass.inputs.ftype = 'LP'\n",
    "smooth_lowpass.iterables = ('fwhm', fwhm)\n",
    "\n",
    "# Spatial Band-Pass Filter\n",
    "smooth_bandpass = Node(Function(input_names=['in_file', 'fwhm', 'ftype'],\n",
    "                                   output_names=['out_file'],\n",
    "                                   function=gaussian_spatial_filter),\n",
    "                          name='smooth_bandpass')\n",
    "smooth_bandpass.inputs.ftype = 'BP'\n",
    "smooth_bandpass.inputs.fwhm = fwhm_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes mean image\n",
    "meanimg = Node(MeanImage(dimension='T',\n",
    "                         output_type='NIFTI_GZ'),\n",
    "               name='meanimg')\n",
    "\n",
    "meanimg_bp = meanimg.clone('meanimg_bp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Main Workflow\n",
    "\n",
    "**Note:** Slice time correction is applied after motion correction, as recommended by Power et al. (2017): http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0182939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main preprocessing workflow\n",
    "mainflow = Workflow(name='mainflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "mainflow.connect([(reorient, nonsteady_detection, [('out_file', 'in_file')]),\n",
    "                  (reorient, nonsteady_removal, [('out_file', 'in_file')]),\n",
    "                  (nonsteady_detection, nonsteady_removal, [('n_volumes_to_discard',\n",
    "                                                             't_min')]),\n",
    "                  (nonsteady_removal, realign, [('roi_file', 'in_files')]),\n",
    "                  (realign, slicetime, [('realigned_files', 'in_files')]),\n",
    "                  (getParam, slicetime, [('TR', 'time_repetition'),\n",
    "                                         ('slice_order', 'slice_order'),\n",
    "                                         ('nslices', 'num_slices'),\n",
    "                                         ('time_acquisition', 'time_acquisition'),\n",
    "                                         ]),\n",
    "                  (slicetime, reset_TR, [('timecorrected_files', 'in_file')]),\n",
    "                  (getParam, reset_TR, [('TR', 'TR')]),\n",
    "                  (reset_TR, bet_func, [('out_file', 'in_file')]),\n",
    "\n",
    "                  # Highpass filter\n",
    "                  (getParam, hpf_sigma, [('TR', 'TR')]),\n",
    "                  (hpf_sigma, hp_filter, [('sigma', 'highpass_sigma')]),\n",
    "                  (bet_func, hp_filter, [('out_file', 'in_file')]),\n",
    "                  (bet_func, hp_mean, [('out_file', 'in_file')]),\n",
    "                  (hp_filter, hp_combine, [('out_file', 'in_file')]),\n",
    "                  (hp_mean, hp_combine, [('out_file', 'operand_file')]),\n",
    "\n",
    "                  # Coregistration\n",
    "                  (coreg_pre, coreg_bbr, [('out_matrix_file', 'in_matrix_file')]),\n",
    "                  (coreg_bbr, applycoreg, [('out_matrix_file', 'in_matrix_file')]),\n",
    "                  (hp_mean, coreg_pre, [('out_file', 'in_file')]),\n",
    "                  (hp_mean, coreg_bbr, [('out_file', 'in_file')]),\n",
    "                  (hp_combine, applycoreg, [('out_file', 'in_file')]),\n",
    "                  (applycoreg, cropper, [('out_file', 'in_file')]),\n",
    "                  \n",
    "                  # Smoothing\n",
    "                  (cropper, smooth_lowpass, [('out_file', 'in_file')]),\n",
    "                  (cropper, smooth_bandpass, [('out_file', 'in_file')]),\n",
    "\n",
    "                  # Create mean image\n",
    "                  (smooth_lowpass, meanimg, [('out_file', 'in_file')]),\n",
    "                  (smooth_bandpass, meanimg_bp, [('out_file', 'in_file')]),\n",
    "                  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the Confound Workflow\n",
    "\n",
    "### Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ACompCor and TCompcor (based on Behzadi et al., 2007)\n",
    "aCompCor = Node(ACompCor(num_components=ncomp,\n",
    "                         pre_filter=False,\n",
    "                         save_pre_filter=False,\n",
    "                         merge_method='union',\n",
    "                         components_file='compcorA.txt'),\n",
    "                name='aCompCor')\n",
    "\n",
    "tCompCor = Node(TCompCor(num_components=ncomp,\n",
    "                         percentile_threshold=0.02,\n",
    "                         pre_filter=False,\n",
    "                         save_pre_filter=False,\n",
    "                         components_file='compcorT.txt'),\n",
    "                name='tCompCor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary mask for ACompCor (based on Behzadi et al., 2007)\n",
    "def get_csf_wm_mask(wm, csf, in_file):\n",
    "    \n",
    "    from nibabel import Nifti1Image\n",
    "    from nilearn.image import threshold_img, resample_to_img\n",
    "    from scipy.ndimage.morphology import binary_erosion, binary_closing\n",
    "\n",
    "    # Create eroded WM binary mask\n",
    "    thr_wm = threshold_img(wm, 0.99)\n",
    "    res_wm = resample_to_img(thr_wm, in_file)\n",
    "    bin_wm = threshold_img(res_wm, 0.5)\n",
    "    mask_wm = binary_erosion(bin_wm.get_fdata(), iterations=2).astype('int8')\n",
    "\n",
    "    # Create eroded CSF binary mask (differs from Behzadi et al., 2007)\n",
    "    thr_csf = threshold_img(csf, 0.99)\n",
    "    res_csf = resample_to_img(thr_csf, in_file)\n",
    "    bin_csf = threshold_img(res_csf, 0.5)\n",
    "    close_csf = binary_closing(bin_csf.get_fdata(), iterations=1)\n",
    "    mask_csf = binary_erosion(close_csf, iterations=1).astype('int8')\n",
    "    \n",
    "    # Combine WM and CSF binary masks into one\n",
    "    binary_mask = ((mask_wm + mask_csf) > 0).astype('int8')\n",
    "    out_file = in_file.replace('.nii', '_maskA.nii')\n",
    "    Nifti1Image(binary_mask, res_wm.affine).to_filename(out_file)\n",
    "\n",
    "    return out_file\n",
    "\n",
    "acomp_masks = Node(Function(input_names=['wm', 'csf', 'in_file'],\n",
    "                            output_names=['out_file'],\n",
    "                            function=get_csf_wm_mask),\n",
    "                   name='acomp_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary mask for TCompCor approach (based on Behzadi et al., 2007)\n",
    "def get_brainmask(in_file):\n",
    "    \n",
    "    from nibabel import Nifti1Image\n",
    "    from nilearn.image import mean_img\n",
    "    from scipy.ndimage.morphology import binary_erosion\n",
    "    \n",
    "    img = mean_img(in_file)\n",
    "    erod_img = binary_erosion(img.get_fdata()>0, iterations=1).astype('int8')\n",
    "    \n",
    "    out_file = in_file.replace('.nii', '_maskT.nii')\n",
    "    Nifti1Image(erod_img, img.affine).to_filename(out_file)\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "tcomp_brainmask = Node(Function(input_names=['in_file'],\n",
    "                          output_names=['out_file'],\n",
    "                          function=get_brainmask),\n",
    "                 name='tcomp_brainmask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Confound Correlation Maps\n",
    "def compute_correlation_map(in_file, confounds, ctype):\n",
    "\n",
    "    import numpy as np\n",
    "    import nibabel as nb\n",
    "    from scipy.stats import zscore\n",
    "\n",
    "    # Load image\n",
    "    img = nb.load(in_file)\n",
    "\n",
    "    # zscore functional data\n",
    "    data = zscore(img.get_fdata(), axis=-1)\n",
    "\n",
    "    # zscore confound parameters\n",
    "    par = zscore(np.loadtxt(\n",
    "        confounds, skiprows='compcor' in confounds), axis=-1)\n",
    "\n",
    "    # Compute correlation map per component\n",
    "    corr_map=np.nan_to_num(np.dot(data, par))\n",
    "\n",
    "    # Extract maximum correlation values per voxel\n",
    "    corr_max = np.abs(corr_map).max(axis=-1)\n",
    "    \n",
    "    # Combine maximum correlation map with component correlation map\n",
    "    corr_comb = np.rollaxis(np.vstack((\n",
    "        corr_max[None,...], np.rollaxis(corr_map, -1, 0))), 0, 4)\n",
    "\n",
    "    # Save and return output file\n",
    "    img = nb.Nifti1Image(corr_comb, img.affine, img.header)\n",
    "    out_file = in_file.replace('.nii', '_corr%s.nii' %ctype)\n",
    "    img.to_filename(out_file)\n",
    "    del img\n",
    "\n",
    "    return out_file\n",
    "\n",
    "comp_corr_mapA = Node(Function(input_names=['in_file', 'confounds', 'ctype'],\n",
    "                               output_names=['out_file'],\n",
    "                               function=compute_correlation_map),\n",
    "                      name='comp_corr_mapA')\n",
    "comp_corr_mapA.inputs.ctype = 'A'\n",
    "comp_corr_mapT = comp_corr_mapA.clone('comp_corr_mapT')\n",
    "comp_corr_mapT.inputs.ctype = 'T'\n",
    "comp_corr_map_rp = comp_corr_mapA.clone('comp_corr_map_rp')\n",
    "comp_corr_map_rp.inputs.ctype = 'RP'\n",
    "comp_corr_map_friston = comp_corr_mapA.clone('comp_corr_map_friston')\n",
    "comp_corr_map_friston.inputs.ctype = 'Friston'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects intensity and motion artifacts and labels them as outliers\n",
    "art = Node(ArtifactDetect(norm_threshold=1,\n",
    "                          zintensity_threshold=2.58, # corresponds to 99%\n",
    "                          mask_type='file',\n",
    "                          parameter_source='SPM',\n",
    "                          use_differences=[True, False],\n",
    "                          plot_type='svg'),\n",
    "           name='art')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes Friston 24-parameter model (Friston et al., 1996)\n",
    "def compute_friston24(in_file):\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    # Load raw motion parameters\n",
    "    mp_raw = np.loadtxt(in_file)\n",
    "    \n",
    "    # Get motion paremter one time point before\n",
    "    mp_minus1 = np.vstack((mp_raw[1:], [0] * 6))\n",
    "    \n",
    "    # Combine the two\n",
    "    mp_combine = np.hstack((mp_raw, mp_minus1))\n",
    "\n",
    "    # Add the square of those parameters to allow correction of nonlinear effects\n",
    "    mp_friston = np.hstack((mp_combine, mp_combine**2))\n",
    "\n",
    "    # Save friston 24-parameter model in new txt file\n",
    "    out_file = in_file.replace('.txt', '_friston24.txt')\n",
    "    np.savetxt(out_file, mp_friston,\n",
    "               fmt='%.8f', delimiter=' ', newline='\\n')\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "friston24 = Node(Function(input_names=['in_file'],\n",
    "                          output_names=['out_file'],\n",
    "                          function=compute_friston24),\n",
    "                 name='friston24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Confound Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confound extraction workflow\n",
    "confflow = Workflow(name='confflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "confflow.connect([(acomp_masks, aCompCor, [('out_file', 'mask_files')]),\n",
    "                  (tcomp_brainmask, tCompCor, [('out_file', 'mask_files')]),\n",
    "\n",
    "                  # Compute confound correlation maps\n",
    "                  (aCompCor, comp_corr_mapA, [('components_file', 'confounds')]),\n",
    "                  (tCompCor, comp_corr_mapT, [('components_file', 'confounds')]),\n",
    "                  (friston24, comp_corr_map_friston, [('out_file', 'confounds')]),\n",
    "                  ])\n",
    "confflow.add_nodes([art, comp_corr_map_rp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subworkflow for the Visualization Workflow\n",
    "\n",
    "### Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Title Text for visualization\n",
    "def create_title(sub, sess, task, run):\n",
    "\n",
    "    return 'Sub: %s - Task: %s - Sess: %s - Run: %s' % (\n",
    "        sub, task, sess, run)\n",
    "\n",
    "get_title = Node(Function(input_names=['sub', 'sess', 'task', 'run'],\n",
    "                          output_names=['title'],\n",
    "                          function=create_title),\n",
    "                 name='get_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize preprocessed functional mean on subject anatomy\n",
    "def plot_mean(anat_file, mean_file, stype, title):\n",
    "\n",
    "    from nibabel import load\n",
    "    from nilearn.plotting import plot_stat_map\n",
    "\n",
    "    # Create correct title\n",
    "    title_text = title + ' - %s' % stype\n",
    "    \n",
    "    # Estimate threshold for nicer visualization\n",
    "    thr = load(mean_file).get_fdata().max() * 0.05\n",
    "\n",
    "    # Plot mean image overlayed on anatomy\n",
    "    out_file = mean_file.replace('.nii.gz', '.svg')\n",
    "    plot_stat_map(\n",
    "        mean_file, title=title_text, bg_img=anat_file, colorbar=False,\n",
    "        annotate=False, display_mode='ortho', threshold=thr, cut_coords=[0, 0, 0],\n",
    "        draw_cross=False, symmetric_cbar= False, output_file=out_file)\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "vis_mean = Node(Function(input_names=['anat_file', 'mean_file',\n",
    "                                      'stype', 'title'],\n",
    "                         output_names=['out_file'],\n",
    "                         function=plot_mean),\n",
    "                name='vis_mean')\n",
    "vis_mean.inputs.stype = 'FWHM'\n",
    "\n",
    "vis_mean_bp = vis_mean.clone('vis_mean_bp')\n",
    "vis_mean_bp.inputs.stype = 'BP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize estimated motion parameters\n",
    "def plot_motion_parameters(mp_file, title):\n",
    "\n",
    "    import seaborn as sns\n",
    "    sns.set(context='notebook', style='darkgrid')\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load parameters and create figure\n",
    "    mp_values = np.loadtxt(mp_file)\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(15, 5))\n",
    "    axes[0].set_ylabel('rotation (radians)')\n",
    "    axes[0].plot(mp_values[0:, 3:])\n",
    "    axes[1].plot(mp_values[0:, :3])\n",
    "    axes[1].set_xlabel('time (TR)')\n",
    "    axes[1].set_ylabel('translation (mm)')\n",
    "    axes[0].set_title(title + ' - Motion Parameters')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save output\n",
    "    out_file = mp_file.replace('.txt','.svg')\n",
    "    fig.savefig(out_file)\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "vis_mp = Node(Function(input_names=['mp_file', 'title'],\n",
    "                       output_names=['out_file'],\n",
    "                       function=plot_motion_parameters),\n",
    "              name='vis_mp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CompCor Masks\n",
    "def plot_compcor_masks(anat_file, mask, mtype, title):\n",
    "\n",
    "    import numpy as np\n",
    "    import nibabel as nb\n",
    "    from matplotlib.pyplot import cm\n",
    "    from nilearn.plotting import plot_roi\n",
    "    from nilearn.image import coord_transform\n",
    "\n",
    "    # Estimate best cut coordinates\n",
    "    img = nb.load(mask)\n",
    "    idx = np.sort(img.get_fdata().nonzero()[1])\n",
    "    if len(idx) != 0:\n",
    "        vox_ids = np.linspace(idx[0], idx[-1], num=12, endpoint=True).astype('int')[2:-2]\n",
    "        cut_ids = [int(coord_transform(0, r, 0, img.affine)[1]) for r in vox_ids]\n",
    "    else:\n",
    "        cut_ids = None\n",
    "\n",
    "    # Visualize mask on anatomy\n",
    "    out_file = mask.replace('.nii.gz', '.svg')\n",
    "    plot_roi(mask, bg_img=anat_file, dim=1, colorbar=False, annotate=False,\n",
    "             threshold=0.5, draw_cross=False, cmap=cm.bwr_r, display_mode='y',\n",
    "             cut_coords=cut_ids, title=title + ' - Mask%s' % mtype,\n",
    "             output_file=out_file)\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "vis_compmaskA = Node(Function(input_names=['anat_file', 'mask', 'mtype', 'title'],\n",
    "                             output_names=['out_file'],\n",
    "                             function=plot_compcor_masks),\n",
    "                    name='vis_compmaskA')\n",
    "vis_compmaskA.inputs.mtype = 'A'\n",
    "\n",
    "vis_compmaskT = vis_compmaskA.clone('vis_compmaskT')\n",
    "vis_compmaskT.inputs.mtype = 'T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize estimated compcor parameters\n",
    "def plot_compcor_parameters(compcor_file, title, cType, ncomp):\n",
    "\n",
    "    import seaborn as sns\n",
    "    sns.set(context='notebook', style='darkgrid')\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Load parameters and create figure\n",
    "    comp = np.loadtxt(compcor_file, skiprows=1)\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    offset = [-r/3. for r in range(ncomp)]\n",
    "    plt.plot(comp + offset)\n",
    "    plt.ylabel('Components')\n",
    "    plt.xlabel('Time [in volumes]')\n",
    "    plt.title(title + 'CompCor%s' % (cType))\n",
    "    plt.yticks(offset, ['comp%02d' % (c + 1) for c in range(ncomp)])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save output\n",
    "    out_file = compcor_file.replace('.txt','.svg')\n",
    "    fig.savefig(out_file)\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "vis_compA = Node(Function(input_names=['compcor_file', 'title',\n",
    "                                       'cType', 'ncomp'],\n",
    "                          output_names=['out_file'],\n",
    "                          function=plot_compcor_parameters),\n",
    "                 name='vis_compA')\n",
    "vis_compA.inputs.cType = 'A'\n",
    "vis_compA.inputs.ncomp = ncomp\n",
    "\n",
    "vis_compT = vis_compA.clone('vis_compT')\n",
    "vis_compT.inputs.cType = 'T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Confound Correlation Maps\n",
    "def plot_confounds(anat_file, corr_file, title, ctype, ncomp):\n",
    "\n",
    "    import numpy as np\n",
    "    from nibabel import load\n",
    "    from nilearn.plotting import plot_stat_map\n",
    "    from nilearn.image import coord_transform\n",
    "    from matplotlib.pyplot import figure\n",
    "\n",
    "    # Create correct title\n",
    "    title_text = title + ' - %s' % ctype\n",
    "    \n",
    "    # Estimate threshold for nicer visualization\n",
    "    img = load(corr_file)\n",
    "    data = img.get_fdata()\n",
    "\n",
    "    # Estimate best cut coordinates\n",
    "    idx = np.sort(data.nonzero()[1])\n",
    "    if len(idx) != 0:\n",
    "        vox_ids = np.linspace(idx[0], idx[-1], num=12, endpoint=True).astype('int')[2:-2]\n",
    "        cut_ids = [int(coord_transform(0, r, 0, img.affine)[1]) for r in vox_ids]\n",
    "    else:\n",
    "        cut_ids = None\n",
    "\n",
    "    # Plot confound correlation map overlayed on anatomy\n",
    "    if ctype[-1] in ['A', 'T']:\n",
    "        nPlots = ncomp\n",
    "    else:\n",
    "        nPlots = 1\n",
    "\n",
    "    fig = figure(figsize=(15, 2 * nPlots))\n",
    "    for i in range(nPlots):\n",
    "        ax = fig.add_subplot(nPlots, 1, i + 1)\n",
    "        comp_img = img.slicer[..., i]\n",
    "        thr = comp_img.get_fdata().max() * 0.25\n",
    "        if i != 0:\n",
    "            postfix = ' - Comp%02d' % i\n",
    "        else:\n",
    "            postfix = ''\n",
    "        \n",
    "        plot_stat_map(\n",
    "            comp_img, title=title_text + postfix, bg_img=anat_file, annotate=False,\n",
    "            colorbar=False, display_mode='y', threshold=thr, draw_cross=False,\n",
    "            cut_coords=cut_ids, symmetric_cbar=False, axes=ax)\n",
    "\n",
    "    # Save output\n",
    "    out_file = corr_file.replace('.nii.gz', '.svg')\n",
    "    fig.savefig(out_file, bbox_inches='tight', facecolor='black', frameon=True)\n",
    "\n",
    "    return out_file\n",
    "\n",
    "vis_corrA = Node(Function(input_names=['anat_file', 'corr_file',\n",
    "                                       'title', 'ctype', 'ncomp'],\n",
    "                         output_names=['out_file'],\n",
    "                         function=plot_confounds),\n",
    "                name='vis_corrA')\n",
    "vis_corrA.inputs.ctype = 'CompCorA'\n",
    "vis_corrA.inputs.ncomp = ncomp\n",
    "vis_corrT = vis_corrA.clone('vis_corrT')\n",
    "vis_corrT.inputs.ctype = 'CompCorT'\n",
    "vis_corr_rp = vis_corrA.clone('vis_corr_rp')\n",
    "vis_corr_rp.inputs.ctype = 'RP'\n",
    "vis_corr_rp.inputs.ncomp = 6\n",
    "vis_corr_friston = vis_corrA.clone('vis_corr_friston')\n",
    "vis_corr_friston.inputs.ctype = 'Friston 24-parameter'\n",
    "vis_corr_friston.inputs.ncomp = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Visualization Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization workflow\n",
    "vizflow = Workflow(name='vizflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "vizflow.connect([(get_title, vis_mp, [('title', 'title')]),\n",
    "                 (get_title, vis_mean, [('title', 'title')]),\n",
    "                 (get_title, vis_mean_bp, [('title', 'title')]),\n",
    "                 (get_title, vis_compmaskA, [('title', 'title')]),\n",
    "                 (get_title, vis_compmaskT, [('title', 'title')]),\n",
    "                 (get_title, vis_compA, [('title', 'title')]),\n",
    "                 (get_title, vis_compT, [('title', 'title')]),\n",
    "                 (get_title, vis_corrA, [('title', 'title')]),\n",
    "                 (get_title, vis_corrT, [('title', 'title')]),\n",
    "                 (get_title, vis_corr_rp, [('title', 'title')]),\n",
    "                 (get_title, vis_corr_friston, [('title', 'title')]),\n",
    "                 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Input & Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over subject, session, task and run id\n",
    "infosource = Node(IdentityInterface(fields=['subject_id', 'session_id',\n",
    "                                            'task_id', 'run_id']),\n",
    "                  name='infosource')\n",
    "infosource.iterables = [('subject_id', subject_list),\n",
    "                        ('session_id', session_list),\n",
    "                        ('task_id', [task_id]),\n",
    "                        ('run_id', run_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify input file location\n",
    "templates = {'anat':  opj('datasink', 'preproc_anat', 'sub-{subject_id}',\n",
    "                          'ses-{session_id}_brain.nii.gz'),\n",
    "             'wm':  opj('datasink', 'preproc_anat', 'sub-{subject_id}',\n",
    "                        'ses-{session_id}_seg_wm.nii'),\n",
    "             'csf':  opj('datasink', 'preproc_anat', 'sub-{subject_id}',\n",
    "                         'ses-{session_id}_seg_csf.nii'),\n",
    "             'func':  opj('/data', 'sub-{subject_id}', 'ses-{session_id}', 'func',\n",
    "                          'sub-{subject_id}_ses-{session_id}_task-{task_id}_run-{run_id}_bold.nii.gz')}\n",
    "\n",
    "sf = Node(SelectFiles(templates,\n",
    "                      base_directory=exp_dir,\n",
    "                      sort_filelist=True),\n",
    "          name='selectfiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save relevant outputs in a datasink\n",
    "datasink = Node(DataSink(base_directory=exp_dir,\n",
    "                         container=out_dir),\n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the following naming substitutions for the datasink\n",
    "substitutions = [('_subject_id_', ''),\n",
    "                 ('_session_id_', ''),\n",
    "                 ('task_id_', ''),\n",
    "                 ('run_id_', '')]\n",
    "substitutions += [('_%s%s%s_%s/' % (run, sess, sub, task_id),\n",
    "                   'sub-%s/task-%s_ses-%s_run-%s_' % (\n",
    "                       sub, task_id, sess, run))\n",
    "                  for sub in subject_list\n",
    "                  for sess in session_list\n",
    "                  for run in run_list]\n",
    "substitutions += [\n",
    "    ('sub-%s_ses-%s_task-%s_run-%s_bold' % (sub, sess, task_id, run), '')\n",
    "    for sub in subject_list\n",
    "    for sess in session_list\n",
    "    for run in run_list]\n",
    "substitutions += [('_ras', ''),\n",
    "                  ('_TR', ''),\n",
    "                  ('_maths', ''),\n",
    "                  ('_filt', ''),\n",
    "                  ('_flirt', ''),\n",
    "                  ('_brain', ''),\n",
    "                  ('_crop', ''),\n",
    "                  ('ar_', ''),\n",
    "                  ('_roi', ''),\n",
    "                  ('rp_', 'rp'),\n",
    "                  ('.r.', '.'),\n",
    "                  ('mask_000', 'maskT'),\n",
    "                  ('.r', ''),\n",
    "                  ('art.r', 'art'),\n",
    "                  ]\n",
    "substitutions += [('_fwhm_%s/' % f, '') for f in fwhm]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implement Functional Preprocessing Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functional preprocessing workflow\n",
    "preproc_func = Workflow(name='preproc_func')\n",
    "preproc_func.base_dir = opj(exp_dir, work_dir)\n",
    "\n",
    "# Connect input nodes to each other\n",
    "preproc_func.connect([(infosource, sf, [('subject_id', 'subject_id'),\n",
    "                                        ('session_id', 'session_id'),\n",
    "                                        ('task_id', 'task_id'),\n",
    "                                        ('run_id', 'run_id')])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input and output nodes and connect them to the main workflow\n",
    "preproc_func.connect([(infosource, mainflow, [('task_id', 'getParam.task_id')]),\n",
    "                      (sf, mainflow, [('func', 'reorient.in_file'),\n",
    "                                      ('anat', 'coreg_pre.reference'),\n",
    "                                      ('anat', 'coreg_bbr.reference'),\n",
    "                                      ('wm', 'coreg_bbr.wm_seg'),\n",
    "                                      ('anat', 'applycoreg.reference')]),\n",
    "                      \n",
    "                      (mainflow, datasink, [\n",
    "                          ('realign.realignment_parameters', 'preproc_func.@realign'),\n",
    "                          ('smooth_lowpass.out_file', 'preproc_func.@func'),\n",
    "                          ('smooth_bandpass.out_file', 'preproc_func.@func_bp'),\n",
    "                          ('meanimg.out_file', 'preproc_func.@mean'),\n",
    "                          ('meanimg_bp.out_file', 'preproc_func.@mean_bp')]),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input and output nodes and connect them to the confound workflow\n",
    "preproc_func.connect([(sf, confflow, [('wm', 'acomp_masks.wm'),\n",
    "                                      ('csf', 'acomp_masks.csf')]),\n",
    "                      \n",
    "                      (confflow, datasink, [\n",
    "                          ('friston24.out_file', 'preproc_func.@friston24'),\n",
    "                          ('art.outlier_files', 'preproc_func.@outlier_files'),\n",
    "                          ('art.plot_files', 'viz_func.@plot_files'),\n",
    "                          ('art.intensity_files', 'preproc_func.@intensity_files'),\n",
    "                          ('art.norm_files', 'preproc_func.@norm_files'),\n",
    "                          ('aCompCor.components_file', 'preproc_func.@compA'),\n",
    "                          ('acomp_masks.out_file', 'preproc_func.@compA_mask'),\n",
    "                          ('tCompCor.components_file', 'preproc_func.@compT'),\n",
    "                          ('tCompCor.high_variance_masks', 'preproc_func.@compT_mask'),\n",
    "                          ('comp_corr_mapA.out_file', 'preproc_func.@corr_compA'),\n",
    "                          ('comp_corr_mapT.out_file', 'preproc_func.@corr_compT'),\n",
    "                          ('comp_corr_map_rp.out_file', 'preproc_func.@corr_compRp'),\n",
    "                          ('comp_corr_map_friston.out_file', 'preproc_func.@corr_compFriston')]),\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input and output nodes and connect them to the visualization workflow\n",
    "preproc_func.connect([(infosource, vizflow, [('subject_id', 'get_title.sub'),\n",
    "                                             ('session_id', 'get_title.sess'),\n",
    "                                             ('task_id', 'get_title.task'),\n",
    "                                             ('run_id', 'get_title.run')]),\n",
    "                      (sf, vizflow, [('anat', 'vis_mean.anat_file'),\n",
    "                                     ('anat', 'vis_mean_bp.anat_file'),\n",
    "                                     ('anat', 'vis_compmaskA.anat_file'),\n",
    "                                     ('anat', 'vis_compmaskT.anat_file'),\n",
    "                                     ('anat', 'vis_corrA.anat_file'),\n",
    "                                     ('anat', 'vis_corrT.anat_file'),\n",
    "                                     ('anat', 'vis_corr_rp.anat_file'),\n",
    "                                     ('anat', 'vis_corr_friston.anat_file')]),\n",
    "                      \n",
    "                      (vizflow, datasink, [\n",
    "                          ('vis_mp.out_file', 'viz_func.@mp'),\n",
    "                          ('vis_mean.out_file', 'viz_func.@vis_mean'),\n",
    "                          ('vis_mean_bp.out_file', 'viz_func.@vis_mean_bp'),\n",
    "                          ('vis_compmaskA.out_file', 'viz_func.@vis_compmaskA'),\n",
    "                          ('vis_compmaskT.out_file', 'viz_func.@vis_compmaskT'),\n",
    "                          ('vis_compA.out_file', 'viz_func.@vis_compA'),\n",
    "                          ('vis_compT.out_file', 'viz_func.@vis_compT'),\n",
    "                          ('vis_corrA.out_file', 'viz_func.@vis_corrA'),\n",
    "                          ('vis_corrT.out_file', 'viz_func.@vis_corrT'),\n",
    "                          ('vis_corr_rp.out_file', 'viz_func.@vis_corrRp'),\n",
    "                          ('vis_corr_friston.out_file', 'viz_func.@vis_corrFriston')]),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect main workflow with confound workflow\n",
    "preproc_func.connect([(mainflow, confflow, [\n",
    "                          ('getParam.TR', 'aCompCor.repetition_time'),\n",
    "                          ('cropper.out_file', 'aCompCor.realigned_file'),\n",
    "                          ('cropper.out_file', 'acomp_masks.in_file'),\n",
    "                          ('getParam.TR', 'tCompCor.repetition_time'),\n",
    "                          ('cropper.out_file', 'tCompCor.realigned_file'),\n",
    "                          ('cropper.out_file', 'tcomp_brainmask.in_file'),\n",
    "                          ('cropper.out_file', 'comp_corr_mapA.in_file'),\n",
    "                          ('cropper.out_file', 'comp_corr_mapT.in_file'),\n",
    "                          ('cropper.out_file', 'comp_corr_map_rp.in_file'),\n",
    "                          ('cropper.out_file', 'comp_corr_map_friston.in_file'),\n",
    "                          ('realign.realigned_files', 'art.realigned_files'),\n",
    "                          ('realign.realignment_parameters', 'art.realignment_parameters'),\n",
    "                          ('realign.realignment_parameters', 'comp_corr_map_rp.confounds'),\n",
    "                          ('realign.realignment_parameters', 'friston24.in_file'),\n",
    "                          ('bet_func.mask_file', 'art.mask_file')]),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect main workflow with visualization workflow\n",
    "preproc_func.connect([(mainflow, vizflow, [\n",
    "                          ('realign.realignment_parameters', 'vis_mp.mp_file'),\n",
    "                          ('meanimg.out_file', 'vis_mean.mean_file'),\n",
    "                          ('meanimg_bp.out_file', 'vis_mean_bp.mean_file')]),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect confound workflow with visualization workflow\n",
    "preproc_func.connect([(confflow, vizflow, [\n",
    "                          ('acomp_masks.out_file', 'vis_compmaskA.mask'),\n",
    "                          ('tCompCor.high_variance_masks', 'vis_compmaskT.mask'),\n",
    "                          ('aCompCor.components_file', 'vis_compA.compcor_file'),\n",
    "                          ('tCompCor.components_file', 'vis_compT.compcor_file'),\n",
    "                          ('comp_corr_mapA.out_file', 'vis_corrA.corr_file'),\n",
    "                          ('comp_corr_mapT.out_file', 'vis_corrT.corr_file'),\n",
    "                          ('comp_corr_map_rp.out_file', 'vis_corr_rp.corr_file'),\n",
    "                          ('comp_corr_map_friston.out_file', 'vis_corr_friston.corr_file')]),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create preproc_func output graph\n",
    "preproc_func.write_graph(graph2use='colored', format='png', simple_form=True)\n",
    "\n",
    "# Visualize the graph\n",
    "from IPython.display import Image\n",
    "Image(filename=opj(preproc_func.base_dir, 'preproc_func', 'graph.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run the workflow in parallel mode\n",
    "preproc_func.run(plugin='MultiProc', plugin_args={'n_procs' : n_proc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save workflow graph visualizations in datasink\n",
    "preproc_func.write_graph(graph2use='flat', format='svg', simple_form=True)\n",
    "preproc_func.write_graph(graph2use='colored', format='svg', simple_form=True)\n",
    "\n",
    "from shutil import copyfile\n",
    "copyfile(opj(preproc_func.base_dir, 'preproc_func', 'graph.svg'),\n",
    "         opj(exp_dir, out_dir, 'preproc_func', 'graph.svg'))\n",
    "copyfile(opj(preproc_func.base_dir, 'preproc_func', 'graph_detailed.svg'),\n",
    "         opj(exp_dir, out_dir, 'preproc_func', 'graph_detailed.svg'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Created Visualizations in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the mean image of the preprocessed functional data\n",
    "for sub in subject_list:\n",
    "    for sess in session_list:\n",
    "        for run in run_list:\n",
    "\n",
    "            for f in fwhm:\n",
    "                display(SVG(opj(\n",
    "                    exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                    'task-%s_ses-%s_run-%s_LP_%smm_mean.svg' % (task_id, sess, run, f))))\n",
    "\n",
    "                display(SVG(opj(\n",
    "                    exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                    'task-%s_ses-%s_run-%s_BP_%smm_mean.svg' % (task_id, sess, run, fwhm_bp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize motion and intensity outliers\n",
    "for sub in subject_list:\n",
    "    for sess in session_list:\n",
    "        for run in run_list:\n",
    "\n",
    "            print('Outlier from: task-%s_ses-%s_run-%s' % (task_id, sess, run))\n",
    "            display(SVG(opj(\n",
    "                exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                'task-%s_ses-%s_run-%s_plot.svg' % (task_id, sess, run))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the masks used for CompCor approach\n",
    "for sub in subject_list:\n",
    "    for sess in session_list:\n",
    "        for run in run_list:\n",
    "\n",
    "            display(SVG(opj(\n",
    "                exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                'task-%s_ses-%s_run-%s_maskA.svg' % (task_id, sess, run))))\n",
    "            display(SVG(opj(\n",
    "                exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                'task-%s_ses-%s_run-%s_maskT.svg' % (task_id, sess, run))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the motion parameters and their correlation with the brain\n",
    "for sub in subject_list:\n",
    "    for sess in session_list:\n",
    "        for run in run_list:\n",
    "\n",
    "            display(SVG(opj(\n",
    "                exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                'task-%s_ses-%s_run-%s_rp.svg' % (task_id, sess, run))))\n",
    "            display(SVG(opj(\n",
    "                exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                'task-%s_ses-%s_run-%s_corrRP.svg' % (task_id, sess, run))))\n",
    "            display(SVG(opj(\n",
    "                exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                'task-%s_ses-%s_run-%s_corrFriston.svg' % (task_id, sess, run))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the CompCor components and their brain correlates\n",
    "for sub in subject_list:\n",
    "    for sess in session_list:\n",
    "        for run in run_list:\n",
    "\n",
    "            display(SVG(opj(\n",
    "                exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                'task-%s_ses-%s_run-%s_compcorA.svg' % (task_id, sess, run))))\n",
    "            display(SVG(opj(\n",
    "                exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                'task-%s_ses-%s_run-%s_corrA.svg' % (task_id, sess, run))))\n",
    "            display(SVG(opj(\n",
    "                exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                'task-%s_ses-%s_run-%s_compcorT.svg' % (task_id, sess, run))))\n",
    "            display(SVG(opj(\n",
    "                exp_dir, out_dir, 'viz_func', 'sub-%s' % sub,\n",
    "                'task-%s_ses-%s_run-%s_corrT.svg' % (task_id, sess, run))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(opj(exp_dir, out_dir, 'preproc_func', 'graph.svg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(opj(exp_dir, out_dir, 'preproc_func', 'graph_detailed.svg')))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
