{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st-level Analysis\n",
    "\n",
    "This notebook performes the 1st-level analysis in subject space by executing the following steps:\n",
    "\n",
    "    1. Aggregate 1st-level model parameters\n",
    "    2. Specify 1st-level contrasts\n",
    "    3. Estimate 1st-level contrasts\n",
    "    4. Normalization to template space (optional)\n",
    "\n",
    "The notebook runs on all functional runs of a particular task, computes the specified beta-contrast and normalizes them into template space. If requested, it will also compute one beta contrast per stimuli occurence (usefull for machine learning approaches). Confounding factors and outlier volumes can be added as nuisance regressors in the GLM.\n",
    "\n",
    "**Note:** This notebook requires that the functional preprocessing pipeline was already executed and that it's output can be found in the dataset folder under `dataset/derivatives/fmriflows/preproc_func`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure Requirements\n",
    "\n",
    "The data structure to run this notebook should be according to the BIDS format:\n",
    "\n",
    "    dataset\n",
    "    ├── analysis-func_specs.json\n",
    "    ├── sub-{sub_id}\n",
    "    │   └── func\n",
    "    │       └── sub-{sub_id}_*task-{task_id}_run-{run_id}_events.tsv\n",
    "    └── derivatives\n",
    "        └── fmriflows\n",
    "            ├── preproc_anat\n",
    "            │   └── sub-{sub_id}\n",
    "            │       └── {sub_id}*_transformComposite.h5\n",
    "            └── preproc_func\n",
    "                └── sub-{sub_id}\n",
    "                    ├── sub-{sub_id}*_task-{task_id}_run-{run_id}_confounds.tsv\n",
    "                    ├── sub-{sub_id}*_task-{task_id}_run-{run_id}_nss.txt\n",
    "                    ├── sub-{sub_id}*_task-{task_id}_run-{run_id}_outliers.txt\n",
    "                    └── sub-{sub_id}*_task-{task_id}_run-{run_id}_tFilter_*_sFilter_*.nii.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Specifications\n",
    "\n",
    "This notebook will extract the relevant processing specifications from the `analysis-1stlevel_specs.json` file in the dataset folder. In the current setup, they are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as opj\n",
    "\n",
    "spec_file = opj('/data', 'analysis-1stlevel_specs.json')\n",
    "\n",
    "with open(spec_file) as f:\n",
    "    specs = json.load(f)\n",
    "\n",
    "specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to change any of those values manually, overwrite them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject names\n",
    "subject_list = specs['subject_list']\n",
    "\n",
    "# Name of task\n",
    "task_id = specs['task_id']\n",
    "\n",
    "# Analysis postfix to use for naming the output folder\n",
    "postfix = specs['analysis_postfix']\n",
    "\n",
    "# Mode and width of spatial filter used during preprocessing,\n",
    "# i.e. Low-Pass, fwhm of 6mm = ['LP', 6]\n",
    "filters_spatial = specs['filters_spatial']\n",
    "\n",
    "# High and low-pass filter used during preprocessing\n",
    "filters_temporal = specs['filters_temporal']\n",
    "\n",
    "# Nuisance regressors to use in GLM\n",
    "nuisance_regressors = specs['nuisance_regressors']\n",
    "\n",
    "# If outliers detected during functional preprocing should be used in GLM\n",
    "use_outliers = specs['use_outliers']\n",
    "\n",
    "# Serial Correlation Model to use\n",
    "model_serial_correlations = specs['model_serial_correlations']\n",
    "\n",
    "# Model bases to use\n",
    "model_bases = {specs['model_bases']['name']: {'derivs': specs['model_bases']['derivs']}}\n",
    "\n",
    "# Estimation Method to use\n",
    "estimation_method = {specs['estimation_method']['name']: specs['estimation_method']['value']}\n",
    "\n",
    "# If contrasts should be normalized to template space\n",
    "normlaize = specs['normalize']\n",
    "\n",
    "# If contrasts should be computed per run\n",
    "con_per_run = specs['con_per_run']\n",
    "\n",
    "# Voxel resolution after normalization\n",
    "norm_res = specs['norm_res']\n",
    "\n",
    "# Number of cores to use\n",
    "n_proc = specs['n_parallel_jobs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TR value\n",
    "import json\n",
    "import numpy as np\n",
    "from os.path import join as opj\n",
    "\n",
    "func_desc = opj('/data', 'task-%s_bold.json' % task_id)\n",
    "\n",
    "with open(func_desc) as f:\n",
    "    func_desc = json.load(f)\n",
    "\n",
    "# Read out relevant parameters\n",
    "TR = func_desc['RepetitionTime']\n",
    "TR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrasts that should be computed according JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of contrasts\n",
    "condition_names = specs['condition_names']\n",
    "condition_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all user specified contrasts\n",
    "con_specs = specs['contrasts']\n",
    "contrast_list = [[c['name'], c['type'], [['T_con_%04d' % i, 'T', condition_names, f] for i, f in enumerate(c['weights'])]] for c in con_specs if c['type'] == 'F']\n",
    "contrast_list += [[c['name'], c['type'], condition_names, c['weights']] for c in con_specs if c['type'] == 'T']\n",
    "contrast_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Workflow\n",
    "\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as opj\n",
    "from nipype import Node, MapNode, Workflow\n",
    "from nipype.interfaces.utility import Function, IdentityInterface\n",
    "from nipype.algorithms.misc import Gunzip\n",
    "from nipype.algorithms.modelgen import SpecifySPMModel\n",
    "from nipype.interfaces.spm import Level1Design, EstimateModel, EstimateContrast\n",
    "from nipype.interfaces.ants import ApplyTransforms\n",
    "from nipype.interfaces.utility import Merge\n",
    "from nipype.interfaces.io import SelectFiles, DataSink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify SPM location\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('/opt/spm12-dev/spm12_mcr/spm/spm12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Execution Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths and names\n",
    "exp_dir = '/data/derivatives'\n",
    "out_dir = 'fmriflows'\n",
    "work_dir = '/workingdir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify 1st-level model parameters (stimuli onsets, duration, etc.)\n",
    "def subjectinfo(subject_id, task_id):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from glob import glob\n",
    "    from nipype.interfaces.base import Bunch\n",
    "    \n",
    "    # Collect names of event files\n",
    "    file_template = '/data/sub-%s/func/' % subject_id\n",
    "    file_template += 'sub-%s_task-%s_run-*_events.tsv' % (subject_id, task_id)\n",
    "    event_files = sorted(glob(file_template))\n",
    "    \n",
    "    # Collect names of non-steady state files\n",
    "    file_template = '/data/derivatives/fmriflows/preproc_func/sub-%s/' % subject_id\n",
    "    file_template += 'sub-%s_task-%s_run-*_nss.txt' % (subject_id, task_id)\n",
    "    nss_files = sorted(glob(file_template))\n",
    "    \n",
    "    subject_info = []\n",
    "    stimuli_order = []\n",
    "    \n",
    "    for i, f in enumerate(event_files):\n",
    "    \n",
    "        trialinfo = pd.read_table(f)\n",
    "        stimuli_order.append(list(trialinfo.condition))\n",
    "        nss = np.loadtxt(nss_files[i])\n",
    "        conditions = []\n",
    "        onsets = []\n",
    "        durations = []\n",
    "        \n",
    "        for group in trialinfo.groupby('condition'):\n",
    "            if group[0] != 'empty':\n",
    "                conditions.append(str(group[0]))\n",
    "                onsets.append(list(group[1].onset - nss))\n",
    "                durations.append(group[1].duration.tolist())\n",
    "\n",
    "        subject_info.append(Bunch(conditions=conditions,\n",
    "                                  onsets=onsets,\n",
    "                                  durations=durations))\n",
    "   \n",
    "    return subject_info, stimuli_order\n",
    "\n",
    "# Get Subject Info - get subject specific condition information\n",
    "getsubjectinfo = Node(Function(input_names=['subject_id', 'task_id'],\n",
    "                               output_names=['subject_info', 'stimuli_order'],\n",
    "                               function=subjectinfo),\n",
    "                      name='getsubjectinfo')\n",
    "getsubjectinfo.inputs.task_id = task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gunzip NIfTI files for SPM\n",
    "gunzip = MapNode(Gunzip(), name='gunzip', iterfield=['in_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SPM model\n",
    "modelspec = Node(SpecifySPMModel(concatenate_runs=False,\n",
    "                                 input_units='secs',\n",
    "                                 output_units='secs',\n",
    "                                 time_repetition=TR,\n",
    "                                 high_pass_filter_cutoff=filters_temporal[0][1]),\n",
    "                 name=\"modelspec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1st-level desing\n",
    "level1design = Node(Level1Design(bases=model_bases,\n",
    "                                 timing_units='secs',\n",
    "                                 interscan_interval=TR,\n",
    "                                 model_serial_correlations=model_serial_correlations),\n",
    "                    name=\"level1design\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate 1st-level model\n",
    "level1estimate = Node(EstimateModel(estimation_method=estimation_method),\n",
    "                      name=\"level1estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate 1st-level contrasts\n",
    "level1conest = Node(EstimateContrast(contrasts=contrast_list),\n",
    "                    name=\"level1conest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge node\n",
    "merge = Node(Merge(2, ravel_inputs=True), name='merge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of template brain with desired voxel resolution\n",
    "template_dir = '/templates/mni_icbm152_nlin_asym_09c/'\n",
    "brain_template = opj(template_dir, '1.0mm_brain.nii.gz')\n",
    "\n",
    "# Resample template brain to desired resolution\n",
    "from nibabel import load, Nifti1Image\n",
    "from nilearn.image import resample_img\n",
    "from nibabel.spaces import vox2out_vox\n",
    "\n",
    "img = load(brain_template)\n",
    "target_shape, target_affine = vox2out_vox(img, voxel_sizes=norm_res)\n",
    "img_resample = resample_img(img, target_affine, target_shape, clip=True)\n",
    "norm_template = opj(template_dir, 'template_brain_%s.nii.gz' %'_'.join([str(n) for n in norm_res]))\n",
    "img_resample.to_filename(norm_template)\n",
    "\n",
    "# Normalize contrasts if requested\n",
    "normfunc = MapNode(ApplyTransforms(reference_image=norm_template,\n",
    "                                   input_image_type=3,\n",
    "                                   float=True,\n",
    "                                   interpolation='LanczosWindowedSinc',\n",
    "                                   invert_transform_flags=[False],\n",
    "                                   out_postfix='_norm'),\n",
    "                   name='normfunc', iterfield=['input_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gzip normalized contrasts\n",
    "def gzip_contrast(contrast):\n",
    "\n",
    "    from nibabel import load\n",
    "    out_file = contrast.replace('.nii', '.nii.gz')\n",
    "    load(contrast).to_filename(out_file)\n",
    "    return out_file\n",
    "\n",
    "gzip = MapNode(Function(input_names=['contrast'],\n",
    "                        output_names=['out_file'],\n",
    "                        function=gzip_contrast),\n",
    "               name='gzip', iterfield=['contrast'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nuisance regressors\n",
    "def create_nuisance_regressors(confounds, nuisance_regressors):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tempfile\n",
    "\n",
    "    # To store regressor files into\n",
    "    regressor_files = []\n",
    "\n",
    "    # Go through confound files\n",
    "    for i, c in enumerate(confounds):\n",
    "        df = pd.read_table(c)\n",
    "        selection = [k for k in df.keys() for n in nuisance_regressors if n in k]\n",
    "        dfs = df[selection]\n",
    "        out_file = tempfile.mkdtemp() + '/confounds_%02d.rst' % i\n",
    "        np.savetxt(out_file, dfs.values)\n",
    "        regressor_files.append(out_file)\n",
    "\n",
    "    return regressor_files\n",
    "\n",
    "nuisance_reg = Node(Function(input_names=['confounds', 'nuisance_regressors'],\n",
    "                             output_names=['confounds'],\n",
    "                             function=create_nuisance_regressors),\n",
    "                      name='nuisance_reg')\n",
    "nuisance_reg.inputs.nuisance_regressors = nuisance_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots design matrix\n",
    "def plot_design_matrix(SPM):\n",
    "\n",
    "    import numpy as np\n",
    "    from matplotlib import pyplot as plt\n",
    "    from scipy.io import loadmat\n",
    "\n",
    "    # Using scipy's loadmat function we can access SPM.mat\n",
    "    spmmat = loadmat(SPM, struct_as_record=False)\n",
    "    \n",
    "    # Now we can load the design matrix and the names of the rows\n",
    "    designMatrix = spmmat['SPM'][0][0].xX[0][0].X\n",
    "    names = [i[0] for i in spmmat['SPM'][0][0].xX[0][0].name[0]]\n",
    "\n",
    "    # Value normalization for better visualization\n",
    "    normed_design = designMatrix / np.abs(designMatrix).max(axis=0)\n",
    "\n",
    "    # Plotting of the design matrix\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    plt.imshow(normed_design, aspect='auto', cmap='gray', interpolation='nearest')\n",
    "    ax.set_ylabel('Volume id')\n",
    "    ax.set_xticks(np.arange(len(names)))\n",
    "    ax.set_xticklabels(names, rotation=90)\n",
    "    design_matrix = SPM.replace('.mat', '.svg')\n",
    "    fig.savefig(design_matrix)\n",
    "    \n",
    "    return design_matrix\n",
    "\n",
    "# Extracts design matrix from SPM.mat and plots it\n",
    "plot_GLM = Node(Function(input_names=['SPM'],\n",
    "                               output_names=['out_file'],\n",
    "                               function=plot_design_matrix),\n",
    "                      name='plot_GLM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots design matrix\n",
    "def plot_contrasts(contrast, template, contrast_names):\n",
    "\n",
    "    import numpy as np\n",
    "    from nibabel import load\n",
    "    from os.path import split\n",
    "    from nilearn.plotting import plot_stat_map\n",
    "    \n",
    "    # Compute 33% percentile for image thresholding\n",
    "    data = np.abs(load(contrast).get_data())\n",
    "    threshold = np.percentile(data[data!=0], 33)\n",
    "\n",
    "    # Create figure\n",
    "    title = contrast_names[int(split(contrast)[-1][4:8]) - 1]\n",
    "    out_file = contrast.replace('.nii.gz', '.svg')\n",
    "    plot_stat_map(contrast, bg_img=template, display_mode='ortho', title=title,\n",
    "                  threshold=threshold, symmetric_cbar=False, annotate=False,\n",
    "                  draw_cross=False, black_bg=True, output_file=out_file)\n",
    "    return out_file\n",
    "\n",
    "# Extracts design matrix from SPM.mat and plots it\n",
    "plot_norm_contrasts = MapNode(Function(input_names=['contrast', 'template',\n",
    "                                                    'contrast_names'],\n",
    "                               output_names=['out_file'],\n",
    "                               function=plot_contrasts),\n",
    "                      name='plot_contrasts', iterfield=['contrast'])\n",
    "plot_norm_contrasts.inputs.template = '/templates/mni_icbm152_nlin_asym_09c/1.0mm_T1.nii.gz'\n",
    "plot_norm_contrasts.inputs.contrast_names = [c[0] for c in contrast_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contrast list for condition per run\n",
    "def get_contrast_per_run(stimuli_order, condition_names):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Aggregate event information\n",
    "    event_list = []\n",
    "    for i, l in enumerate(stimuli_order):\n",
    "        event_list.append([z for z in zip(np.full(len(l), i), l)])\n",
    "    event_info = np.reshape(event_list, (-1, 2))\n",
    "\n",
    "    unique_contrasts = np.unique(event_info[:,1])\n",
    "    n_runs = np.unique(event_info[:,0])\n",
    "\n",
    "    # Create list of contrasts for each condition per run\n",
    "    contrast_list_run = []\n",
    "    n_contrasts = len(condition_names)\n",
    "    n_conditions = len(n_runs)\n",
    "    condition_labels = []\n",
    "\n",
    "    for j in range(n_conditions):\n",
    "        for i in range(n_contrasts):\n",
    "            name = 'cont_%05d' % (1 + i + j * n_conditions)\n",
    "            con_id = np.zeros(n_contrasts).tolist()\n",
    "            run_id = np.zeros(n_conditions).tolist()\n",
    "            con_id[i] = 1\n",
    "            run_id[j] = 1\n",
    "            contrast_list_run.append([\n",
    "                name, 'T', condition_names, con_id, run_id])\n",
    "            condition_labels.append(condition_names[i])\n",
    "    \n",
    "    return contrast_list_run, condition_labels\n",
    "\n",
    "# Extracts design matrix from SPM.mat and plots it\n",
    "contrast_per_run = Node(Function(input_names=['stimuli_order', 'condition_names'],\n",
    "                                     output_names=['run_contrasts', 'condition_labels'],\n",
    "                                     function=get_contrast_per_run),\n",
    "                            name='contrast_per_run')\n",
    "contrast_per_run.inputs.condition_names = condition_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Input & Output Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over subject and session id\n",
    "infosource = Node(IdentityInterface(fields=['subject_id']),\n",
    "                  name='infosource')\n",
    "infosource.iterables = [('subject_id', subject_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Brain Mask and Extract Brain\n",
    "def create_file_path(sub_id, task_id, tFilter, sFilter):\n",
    "\n",
    "    from os.path import join\n",
    "    from glob import glob\n",
    "    from os.path import join as opj\n",
    "    \n",
    "    # Path to preprocessing folders\n",
    "    path_anat = '/data/derivatives/fmriflows/preproc_anat/'\n",
    "    path_func = '/data/derivatives/fmriflows/preproc_func/'\n",
    "\n",
    "    # tFilter Id\n",
    "    t_id = '.'.join([str(t) for t in tFilter])\n",
    "    s_id = '_'.join([str(t) for t in sFilter]) + 'mm'\n",
    "    \n",
    "    transforms = glob(opj(path_anat, 'sub-%s' % sub_id,\n",
    "                          'sub-%s_transformComposite.h5' % sub_id))[0]\n",
    "    func = sorted(glob(opj(path_func, 'sub-%s' % sub_id,\n",
    "                           'sub-%s_task-%s_run-*tFilter_%s_sFilter_%s.nii.gz' % (sub_id, task_id, t_id, s_id))))\n",
    "    outliers = sorted(glob(opj(path_func, 'sub-%s' % sub_id,\n",
    "                               'sub-%s_task-%s_run-*outliers.txt' % (sub_id, task_id))))\n",
    "    confounds = sorted(glob(opj(path_func, 'sub-%s' % sub_id,\n",
    "                                'sub-%s_task-%s_run-*confounds.tsv' % (sub_id, task_id))))\n",
    "    \n",
    "    return transforms, func, outliers, confounds\n",
    "\n",
    "selectfiles = Node(Function(input_names=['sub_id','task_id','tFilter','sFilter'],\n",
    "                            output_names=['transforms', 'func', 'outliers', 'confounds'],\n",
    "                            function=create_file_path),\n",
    "                   name='selectfiles')\n",
    "selectfiles.iterables = [('tFilter', filters_temporal),\n",
    "                         ('sFilter', filters_spatial)]\n",
    "selectfiles.inputs.task_id = task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save relevant outputs in a datasink\n",
    "datasink = Node(DataSink(base_directory=exp_dir,\n",
    "                         container=out_dir),\n",
    "                name='datasink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the following naming substitutions for the datasink\n",
    "substitutions = [('_subject_id_%s/_' % sub,\n",
    "                  '%s/sub-%s/sub-%s_' % (postfix, sub, sub))\n",
    "                 for sub in subject_list]\n",
    "substitutions += [('sub-%s_sFilter_%s_tFilter_%s/' % (sub, s, t),\n",
    "                   'sub-%s_sFilter_%s_tFilter_%s_' % (sub, s, t))\n",
    "                  for sub in subject_list\n",
    "                  for t in ['.'.join([str(t) for t in tFilter]) for tFilter in filters_temporal]\n",
    "                  for s in ['.'.join([str(t) for t in sFilter]) for sFilter in filters_spatial]]\n",
    "substitutions += [('_normfunc%d/' % c, '') for c in range(len(contrast_list))]\n",
    "substitutions += [('_normfunc_run%d/' % c, '') for c in range(1000)]\n",
    "substitutions += [('multivariate/%s' % postfix, '%s/multivariate' % postfix)]\n",
    "substitutions += [('univariate/%s' % postfix, '%s/univariate' % postfix)]\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 1st-Level Analysis Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anatomical preprocessing workflow\n",
    "analysis_1st = Workflow(name='analysis_1st')\n",
    "analysis_1st.base_dir = work_dir\n",
    "output_folder = 'analysis_1stLevel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to workflow and connect them\n",
    "analysis_1st.connect([(infosource, selectfiles, [('subject_id', 'sub_id')]),\n",
    "                      (infosource, getsubjectinfo, [('subject_id', 'subject_id')]),\n",
    "                      (getsubjectinfo, modelspec, [('subject_info', 'subject_info')]),\n",
    "                      (selectfiles, gunzip, [('func', 'in_file')]),\n",
    "                      (gunzip, modelspec, [('out_file', 'functional_runs')]),\n",
    "                      (modelspec, level1design, [('session_info', 'session_info')]),\n",
    "                      (level1design, level1estimate, [('spm_mat_file', 'spm_mat_file')]),\n",
    "                      (level1estimate, level1conest, [('spm_mat_file', 'spm_mat_file'),\n",
    "                                                      ('beta_images', 'beta_images'),\n",
    "                                                      ('residual_image', 'residual_image')]),\n",
    "                      (selectfiles, nuisance_reg, [('confounds', 'confounds')]),\n",
    "                      (nuisance_reg, modelspec, [('confounds', 'realignment_parameters')]),\n",
    "\n",
    "                      # Store main results in datasink\n",
    "                      (level1conest, datasink, [('spm_mat_file', '%s.univariate.@spm_mat' % output_folder),\n",
    "                                                ('con_images', '%s.univariate.@con' % output_folder),\n",
    "                                                ('ess_images', '%s.univariate.@ess' % output_folder),\n",
    "                                                ('spmT_images', '%s.univariate.@spmT' % output_folder),\n",
    "                                                ('spmF_images', '%s.univariate.@spmF' % output_folder)]),\n",
    "\n",
    "                      # Create visual outputs and report\n",
    "                      (level1conest, plot_GLM, [('spm_mat_file', 'SPM')]),\n",
    "                      (plot_GLM, datasink, [('out_file', '%s.univariate.@spm_mat_svg' % output_folder)]),\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add outlier parameters if requested by user\n",
    "if use_outliers:\n",
    "    analysis_1st.connect([(selectfiles, modelspec, [('outliers', 'outlier_files')])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize contrasts to template space if requested by user\n",
    "if normlaize:\n",
    "    analysis_1st.connect([(level1conest, merge, [('con_images', 'in1'),\n",
    "                                                 ('ess_images', 'in2')]),\n",
    "                          (merge, normfunc, [('out', 'input_image')]),\n",
    "                          (selectfiles, normfunc, [('transforms', 'transforms')]),\n",
    "                          (normfunc, gzip, [('output_image', 'contrast')]),\n",
    "                          (gzip, plot_norm_contrasts, [('out_file', 'contrast')]),\n",
    "                          \n",
    "                          (gzip, datasink, [('out_file', '%s.univariate.@norm_files' % output_folder)]),\n",
    "                          (plot_norm_contrasts, datasink, [('out_file',\n",
    "                                                            '%s.univariate.@norm_plot' % output_folder)]),\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create workflow if contrasts per condition per run should be computed\n",
    "if con_per_run:\n",
    "    \n",
    "    # Estimate 1st-level contrasts - one for each session\n",
    "    level1conest_run = Node(EstimateContrast(), name=\"level1conest_run\")\n",
    "    \n",
    "    # Normalize contrasts\n",
    "    normfunc_run = normfunc.clone('normfunc_run')\n",
    "    \n",
    "    # Gzip contrasts\n",
    "    gzip_run = gzip.clone('gzip_run')\n",
    "    \n",
    "    # Write label file\n",
    "    def write_labels_file(condition_labels, spm_mat_file):\n",
    "\n",
    "        import numpy as np\n",
    "        label_file = spm_mat_file.replace('SPM.mat', 'labels.csv')\n",
    "        np.savetxt(label_file, condition_labels, fmt='%s')\n",
    "\n",
    "        return label_file\n",
    "    \n",
    "    write_labels = Node(Function(input_names=['condition_labels', 'spm_mat_file'],\n",
    "                                     output_names=['labels_file'],\n",
    "                                     function=write_labels_file),\n",
    "                            name='write_labels')\n",
    "\n",
    "    # Connect all nodes in this part of the workflow\n",
    "    analysis_1st.connect([(getsubjectinfo, contrast_per_run, [('stimuli_order', 'stimuli_order')]),\n",
    "                          (contrast_per_run, level1conest_run, [('run_contrasts', 'contrasts')]),\n",
    "                          (level1estimate, level1conest_run, [('spm_mat_file', 'spm_mat_file'),\n",
    "                                                              ('beta_images', 'beta_images'),\n",
    "                                                              ('residual_image', 'residual_image')]),\n",
    "                          (level1conest_run, normfunc_run, [('con_images', 'input_image')]),\n",
    "                          (selectfiles, normfunc_run, [('transforms', 'transforms')]),\n",
    "                          (normfunc_run, gzip_run, [('output_image', 'contrast')]),\n",
    "                          (gzip_run, datasink, [('out_file', '%s.multivariate.@norm_files' % output_folder)]),\n",
    "                          (level1estimate, write_labels, [('spm_mat_file', 'spm_mat_file')]),\n",
    "                          (contrast_per_run, write_labels, [('condition_labels', 'condition_labels')]),\n",
    "                          (write_labels, datasink, [('labels_file', '%s.multivariate.@norm_labels' % output_folder)]),\n",
    "                         ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create preproc_anat output graph\n",
    "analysis_1st.write_graph(graph2use='colored', format='svg', simple_form=True)\n",
    "\n",
    "# Visualize the graph in the notebook\n",
    "from IPython.display import SVG\n",
    "SVG(filename=opj(analysis_1st.base_dir, 'analysis_1st', 'graph.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run the workflow in parallel mode\n",
    "analysis_1st.run(plugin='MultiProc', plugin_args={'n_procs' : n_proc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save workflow graph visualizations in datasink\n",
    "analysis_1st.write_graph(graph2use='flat', format='svg', simple_form=True)\n",
    "analysis_1st.write_graph(graph2use='colored', format='svg', simple_form=True)\n",
    "\n",
    "from shutil import copyfile\n",
    "copyfile(opj(analysis_1st.base_dir, 'analysis_1st', 'graph.svg'),\n",
    "         opj(exp_dir, out_dir,  output_folder, postfix, 'graph.svg'))\n",
    "copyfile(opj(analysis_1st.base_dir, 'analysis_1st', 'graph_detailed.svg'),\n",
    "         opj(exp_dir, out_dir, output_folder, postfix, 'graph_detailed.svg'));"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
