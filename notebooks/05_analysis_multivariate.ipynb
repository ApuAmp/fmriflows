{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <p style=\"float: right;\"><img width=\"66%\" src=\"templates/logo_fmriflows.gif\"></p>\n",
    "    <h1>Multivariate Analysis</h1>\n",
    "    <p>This notebook performes a simple multivariate analysis by executing the following steps:\n",
    "\n",
    "1. Collect files and labels\n",
    "1. Create mask to restrict searchlight analysis\n",
    "1. Create PyMVPA dataset and z-score it\n",
    "1. Select target samples\n",
    "1. Run searchlight analysis\n",
    "1. Create outputs 1st-level\n",
    "1. Perform 2nd-level analysis (classical GLM and/or multivariate according to [Stelzer et al. (2013)](https://www.sciencedirect.com/science/article/pii/S1053811912009810))\n",
    "\n",
    "**Note:** This notebook requires that the 1st-level analysis pipeline was already executed, with the parameter `con_per_run` set to `True`, and that it's output can be found in the dataset folder under `/dataset/derivatives/fmriflows/analysis_1stLevel/multivariate`. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structure Requirements\n",
    "\n",
    "The data structure to run this notebook should be according to the BIDS format:\n",
    "\n",
    "    dataset\n",
    "    ├── fmriflows_spec_multivariate.json\n",
    "    └── derivatives\n",
    "        └── fmriflows\n",
    "            └── analysis_1stLevel\n",
    "                └── multivariate\n",
    "                    └── sub-{sub_id}\n",
    "                        └── task-{task_id}\n",
    "                            └── tFilter-{tFilter_id}_sFilter-{sFilter_id}\n",
    "                                ├── con_[con_id]_norm.nii.gz\n",
    "                                └── labels.csv\n",
    "\n",
    "`fmriflows` will perform a multivariate (searchlight) analysis on each subjects individually. Type of classifier and possible binary classifications need to be specified in the `fmriflows_spec_multivariate.json` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Specifications\n",
    "\n",
    "This notebook will extract the relevant analysis specifications from the `fmriflows_spec_multivariate.json` file in the dataset folder. In the current setup, they are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import join as opj\n",
    "\n",
    "spec_file = opj('/data', 'fmriflows_spec_multivariate.json')\n",
    "\n",
    "with open(spec_file) as f:\n",
    "    specs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameters for 1st-level analysis workflow\n",
    "subject_list = specs['subject_list']\n",
    "session_list = specs['session_list']\n",
    "filters_spatial = specs['filters_spatial']\n",
    "filters_temporal = specs['filters_temporal']\n",
    "gm_mask_thr = specs['gm_mask_thr']\n",
    "postfix = specs['multivariate_postfix']\n",
    "clf_names = specs['clf_names']\n",
    "sphere_radius = specs['sphere_radius']\n",
    "sphere_steps = specs['sphere_steps']\n",
    "n_chunks = specs['n_chunks']\n",
    "tasks = specs['tasks']\n",
    "n_perm = specs['n_perm']\n",
    "atlasreader_names = specs['atlasreader_names']\n",
    "atlasreader_prob_thresh = specs['atlasreader_prob_thresh']\n",
    "n_proc = specs['n_parallel_jobs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to change any of those values manually, overwrite them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subject identifiers\n",
    "subject_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of session identifiers\n",
    "session_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of spatial filters (smoothing) that were used during functional preprocessing\n",
    "filters_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of temporal filters that were used during functional preprocessing\n",
    "filters_temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value to threshold gray matter probability template to create 2nd-level mask\n",
    "gm_mask_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a particular analysis postfix\n",
    "postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of classifier to use. Choose one or many of:\n",
    "#   'LinearCSVMC', 'LinearNuSVMC', 'RbfCSVMC', 'RbfNuSVMC', 'SMLR', 'kNN', 'GNB'\n",
    "clf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searchlight sphere radius (in voxels), i.e. number of additional voxels\n",
    "#    next to the center voxel. E.g sphere_radius = 3 means radius = 3.5*voxelsize\n",
    "sphere_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of step size to define a sphere center, i.e. value of 5 means\n",
    "#    that only every 5th voxel is used to perform a searchlight analysis\n",
    "sphere_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of chunks to use for the N-Fold crossvalidation\n",
    "#    (needs to divide number of labels without reminder)\n",
    "n_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which classifications should be performed? (separated by task)\n",
    "#    - Classification targets are a tuple of two tuples, indicating\n",
    "#    - Target classification to train and target classification to test\n",
    "tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of permutations to indicate group-analysis strategy:\n",
    "#    n_perm = 0: group analysis is classical 2nd-level GLM analysis\n",
    "#    n_perm > 0: group analysis is multivariate analysis according to Stelzer et al. (2013)\n",
    "n_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of atlases to use for creation of output tables\n",
    "atlasreader_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability threshold to use for output tables\n",
    "atlasreader_prob_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of parallel jobs to run\n",
    "n_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations\n",
    "\n",
    "First things first, let's import important modules and specify relevant environment variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os.path import basename\n",
    "from mvpa2.suite import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths and names\n",
    "exp_dir = '/data/derivatives'\n",
    "out_dir = 'fmriflows'\n",
    "work_dir = '/workingdir'\n",
    "\n",
    "# Create multivariate output workflow\n",
    "out_folder_name = 'analysis_multivariate'\n",
    "if postfix:\n",
    "    out_folder_name += '_%s' % postfix\n",
    "out_path = opj(exp_dir, out_dir, out_folder_name)\n",
    "\n",
    "# Create output folder\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition\n",
    "\n",
    "In this section, we will define all the functions that we need to run the searchlight analysis.\n",
    "\n",
    "## Collect files\n",
    "\n",
    "This function will return a list containing the beta-maps, a list containing the corresponding labels and a list that specifies which labels and contrasts belong to which chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_files(subject_id, session_id, task_id, tFilter_id, sFilter_id, n_chunks):\n",
    "\n",
    "    \"\"\"This function collects the relevant input files, labels and chunk_idx\"\"\"\n",
    "    \n",
    "    # Get normalized 1st-level contrasts for specific parameters\n",
    "    from bids.layout import BIDSLayout\n",
    "    layout = BIDSLayout('/data/')\n",
    "\n",
    "    # Collect contrast files\n",
    "    search_parameters = {'subject': subject_id,\n",
    "                         'task': task_id,\n",
    "                         'return_type': 'file',\n",
    "                         'type': 'norm'\n",
    "                        }\n",
    "\n",
    "    if session_id:\n",
    "        search_parameters['session'] = session_id\n",
    "\n",
    "    norm_files = layout.get(**search_parameters)\n",
    "\n",
    "    # Collect label file\n",
    "    search_parameters['type'] = 'labels'\n",
    "    label_files = layout.get(**search_parameters)\n",
    "\n",
    "    # Collect only requested files\n",
    "    pass1 = '/data/derivatives/fmriflows/analysis_1stLevel'\n",
    "    pass2 = '/multivariate'\n",
    "\n",
    "    con_files = []\n",
    "    labels = []\n",
    "\n",
    "    for c in norm_files:\n",
    "        if pass1 in c and pass2 in c:\n",
    "            if tFilter_id in c and sFilter_id in c:\n",
    "                con_files.append(c)\n",
    "\n",
    "    for l in label_files:\n",
    "        if pass1 in l and pass2 in l:\n",
    "            if 'tFilter_' + tFilter_id in l and 'sFilter_' + sFilter_id in l:\n",
    "                labels += list(np.loadtxt(l, dtype='S'))\n",
    "                \n",
    "    # Create chunks index\n",
    "    chunks = [i for i in range(n_chunks)\n",
    "              for j in range(int(len(con_files) / n_chunks))]\n",
    "    \n",
    "    # Make sure that that you have same number of contrasts and labels\n",
    "    assert(len(con_files)==len(labels)==len(chunks))\n",
    "    \n",
    "    return sorted(con_files), labels, chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify mask\n",
    "\n",
    "This function will create and return a binary mask from the gray matter probability template to restrict the searchlight analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gray matter probability template to use for mask creation\n",
    "mask_file = '/templates/mni_icbm152_nlin_asym_09c/1.0mm_tpm_gm.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(mask_file, con_file, gm_mask_thr, out_file):\n",
    "    \n",
    "    \"\"\"Creates and returns a gray matter mask, that can be applied before\n",
    "    the searchlight analysis.\"\"\"\n",
    "\n",
    "    from nilearn.image import resample_to_img, math_img, new_img_like\n",
    "    from scipy.ndimage.morphology import binary_dilation\n",
    "\n",
    "    # Resample mask image to contrast space and rescale to range of [0, 1]\n",
    "    img_mask = resample_to_img(mask_file, con_file)\n",
    "    mask = math_img('img/np.max(img) >= {}'.format(gm_mask_thr),\n",
    "                    img=img_mask).get_data()\n",
    "\n",
    "    # Apply binary dilation to image\n",
    "    mask = binary_dilation(mask, iterations=2)\n",
    "    img_mask = new_img_like(img_mask, mask, img_mask.affine)\n",
    "\n",
    "    # Save image as a NIfTI file\n",
    "    img_mask.to_filename(out_file)\n",
    "    \n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searchlight results function\n",
    "\n",
    "This function takes the results aggregated from the searchlight analysis and stores in every voxel of the mask the average value of each sphere that included this particular voxel. This function has therefore an effect of **smoothing the results**. Additionally, this function also allows to **fill up wholes** in the searchlight map if not every voxel was used as a center of a sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_scattered_results(sl, dataset, roi_ids, results):\n",
    "\n",
    "    \"\"\"Function to aggregate results - This requires the searchlight\n",
    "    conditional attribute 'roi_feature_ids' to be enabled\"\"\"\n",
    "    \n",
    "    resmap = None\n",
    "    for resblock in results:\n",
    "        for res in resblock:\n",
    "            if resmap is None:\n",
    "                \n",
    "                # prepare the result container\n",
    "                resmap = np.zeros((len(res), dataset.nfeatures),\n",
    "                                  dtype=res.samples.dtype)\n",
    "                observ_counter = np.zeros(dataset.nfeatures, dtype=int)\n",
    "            \n",
    "            # project the result onto all features -- love broadcasting!\n",
    "            resmap[:, res.a.roi_feature_ids] += res.samples\n",
    "            \n",
    "            # increment observation counter for all relevant features\n",
    "            observ_counter[res.a.roi_feature_ids] += 1\n",
    "    \n",
    "    # when all results have been added up average them according to the number\n",
    "    # of observations\n",
    "    observ_mask = observ_counter > 0\n",
    "    resmap[:, observ_mask] /= observ_counter[observ_mask]\n",
    "    result_ds = Dataset(resmap,\n",
    "                        fa={'observations': observ_counter})\n",
    "    \n",
    "    if 'mapper' in dataset.a:\n",
    "        import copy\n",
    "        result_ds.a['mapper'] = copy.copy(dataset.a.mapper)\n",
    "    \n",
    "    return result_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier selection\n",
    "\n",
    "This function return the classifier object defined by the classifier name `clf_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifier(clf_name):\n",
    "    \n",
    "    \"\"\"Returns specified classifier object\"\"\"\n",
    "    clfs = {\n",
    "        'LinearCSVMC': LinearCSVMC(),\n",
    "        'LinearNuSVMC': LinearNuSVMC(),\n",
    "        'RbfCSVMC': RbfCSVMC(),\n",
    "        'RbfNuSVMC': RbfNuSVMC(),\n",
    "        'SMLR': SMLR(),\n",
    "        'kNN': kNN(k=3),\n",
    "        'GNB': GNB(),\n",
    "    }\n",
    "\n",
    "    return clfs[clf_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMVPA dataset creation\n",
    "\n",
    "This function creates the dataset object `ds` needed for the searchlight analysis. This is also where the data is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.base.hdf5 import h5save\n",
    "from mvpa2.datasets.mri import fmri_dataset\n",
    "from mvpa2.mappers.zscore import zscore\n",
    "\n",
    "def create_dataset(subject_id, session_id, task_id, tFilter, sFilter, n_chunks, out_path, mask_file):\n",
    "    \n",
    "    \"\"\"Create PyMVPA dataset for given input parameters and stores it in a HDF5 file\"\"\"\n",
    "    \n",
    "    # Create filter idx\n",
    "    tFilter_id = '%s.%s' % (tFilter[0], tFilter[1])\n",
    "    sFilter_id = '%s.%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "    # Collect files, labels and chunks\n",
    "    con_files, labels, chunks = collect_files(subject_id, session_id, task_id,\n",
    "                                              tFilter_id, sFilter_id, n_chunks)\n",
    "    \n",
    "    # Create binary gray matter mask\n",
    "    out_file = opj(out_path, 'gm_mask.nii.gz')\n",
    "    mask_img = create_mask(mask_file, con_files[0], gm_mask_thr, out_file)\n",
    "    \n",
    "    # Create dataset\n",
    "    ds = fmri_dataset(samples=con_files,\n",
    "                      targets=labels,\n",
    "                      chunks=chunks,\n",
    "                      mask=mask_img)\n",
    "    del ds.sa['time_coords']\n",
    "    del ds.sa['time_indices']\n",
    "\n",
    "    # Normalize dataset\n",
    "    zscore(ds)\n",
    "    \n",
    "    # Save dataset in HDF5 format and return in\n",
    "    ds_name = 'sub-%s_tFilter-%s_sFilter-%s_mask-%.03f.hdf5' % (\n",
    "        subject_id, tFilter_id, sFilter_id, gm_mask_thr)\n",
    "    if session_id:\n",
    "        ds_name.replace('_tFilter', 'ses-%s_tFilter' % session_id)\n",
    "    ds_path = opj(out_path, 'task-%s' % task_id, ds_name)\n",
    "    h5save(ds_path, ds)\n",
    "    \n",
    "    return ds_path, mask_img, ds_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create searchlight specific dataset\n",
    "\n",
    "This function takes the PyMVPA dataset and prepares it for the searchlight analysis. In particular, it removes the target labels that are not needed for the classification and prepares the dataset for the cross-classification (training on one group to predict another) or the standard classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(ds_path, train_labels, test_labels, cross_clf):\n",
    "\n",
    "    \"\"\"Prepares the dataset for the searchlight classification\"\"\"\n",
    "\n",
    "    # Load dataset and extract labels and chunks\n",
    "    ds = h5load(ds_path)\n",
    "    labels = np.copy(ds.targets)\n",
    "    chunks = np.copy(ds.chunks)\n",
    "    \n",
    "    # Select targets and rename chunks and labels if necessary\n",
    "    if cross_clf:\n",
    "        selecter = np.isin(labels, train_labels + test_labels)\n",
    "        chunks = [int((l in test_labels) and ((l not in train_labels))) for l in labels[selecter]]\n",
    "        selection = labels[selecter]\n",
    "        for i, l in enumerate(test_labels):\n",
    "            selection[np.argwhere(selection==test_labels[i])] = train_labels[i]\n",
    "    else:\n",
    "        selecter = np.isin(labels, train_labels)\n",
    "        chunks = chunks[selecter]\n",
    "        selection = labels[selecter]\n",
    "\n",
    "    # Create searchlight analysis specifc dataset\n",
    "    ds_sl = ds[selecter]\n",
    "    ds_sl.sa.chunks = chunks\n",
    "    ds_sl.sa.targets = selection\n",
    "\n",
    "    return ds_sl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run searchlight analysis\n",
    "\n",
    "This function runs the searchlight analysis with the given input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.generators.partition import NFoldPartitioner\n",
    "from mvpa2.measures.base import CrossValidation\n",
    "from mvpa2.measures.searchlight import sphere_searchlight\n",
    "from mvpa2.misc.errorfx import mean_match_accuracy\n",
    "from mvpa2.mappers.fx import mean_sample\n",
    "from mvpa2.base import debug\n",
    "\n",
    "def run_searchlight(ds_sl, clf_name, cross_clf, sphere_radius, sphere_steps, n_proc, verbose=False):\n",
    "\n",
    "    \"\"\"Run Searchlight Analysis and return searchlight output\"\"\"\n",
    "    \n",
    "    # Specify cross-validation scheme\n",
    "    partitioner = NFoldPartitioner(cvtype=1)\n",
    "    \n",
    "    # Specify classifier\n",
    "    clf = get_classifier(clf_name)\n",
    "    \n",
    "    # Create cross validation object\n",
    "    if cross_clf:\n",
    "        cv = CrossValidation(clf,\n",
    "                             partitioner,\n",
    "                             errorfx=mean_match_accuracy,\n",
    "                             enable_ca=['stats'],\n",
    "                             splitter=Splitter(attr='chunks',\n",
    "                                               attr_values=(0, 1)))\n",
    "    else:\n",
    "        cv = CrossValidation(clf,\n",
    "                             partitioner,\n",
    "                             errorfx=mean_match_accuracy,\n",
    "                             enable_ca=['stats'])\n",
    "\n",
    "    # Create searchlight object\n",
    "    sl = sphere_searchlight(cv,\n",
    "                            radius=sphere_radius,\n",
    "                            center_ids=range(0,\n",
    "                                             ds_sl.shape[1],\n",
    "                                             sphere_steps),\n",
    "                            space='voxel_indices',\n",
    "                            results_fx=fill_in_scattered_results,\n",
    "                            postproc=mean_sample(),\n",
    "                            enable_ca=['calling_time', 'roi_feature_ids'],\n",
    "                            nproc=n_proc)\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    # Turn verbose on or off\n",
    "    if verbose:\n",
    "        debug.active = [\"SLC\"]\n",
    "    else:\n",
    "        debug.active = []\n",
    "    \n",
    "    # Run searchlight analysis\n",
    "    sl_map = sl(ds_sl)\n",
    "    \n",
    "    return sl, sl_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create informative logfile\n",
    "\n",
    "This function writes relevant searchlight analysis information to a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.suite import time\n",
    "\n",
    "def create_output(ds, sl, sl_map, subject_id, clf_name, targets, sphere_radius,\n",
    "                  sphere_steps, ds_name, clf_type, result_path, n_proc):\n",
    "    \n",
    "    \"\"\"Save important model information in a text file.\"\"\"\n",
    "\n",
    "    # Extract important information\n",
    "    wall_time = time.strftime('%H:%M:%S', time.gmtime(round(sl.ca.calling_time)))\n",
    "\n",
    "    # Accuracy information\n",
    "    accuracies = sl_map.S[0]\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    std_accuracy = np.std(accuracies)\n",
    "    chance_level = 1.0 / len(np.unique(ds.sa.targets))\n",
    "\n",
    "    # Helper functions\n",
    "    def threshold_above_average(x):\n",
    "        return chance_level + x * std_accuracy\n",
    "\n",
    "    def spheres_above_average(x):\n",
    "        return np.sum(accuracies >= threshold_above_average(x))\n",
    "\n",
    "    def percent_above_average(x):\n",
    "        return np.mean(accuracies >= threshold_above_average(x)) * 100\n",
    "    \n",
    "    # Output text\n",
    "    txt = ['Subject          : {0}'.format(subject_id),\n",
    "           'Classifier       : {0}'.format(clf_name),\n",
    "           'Classes          : {0}'.format(targets),\n",
    "           'Sphere Radius    : {0}'.format(sphere_radius),\n",
    "           'N-th Element     : {0}'.format(sphere_steps),\n",
    "           'Wall Time        : {0}'.format(wall_time),\n",
    "           'Samples          : {0}'.format(ds.S.shape[0]),\n",
    "           'Features         : {0}'.format(ds.S.shape[1]),\n",
    "           'Volume Dimension : {0}'.format(str(ds.a.voxel_dim)),\n",
    "           'Voxel  Dimension : {0}'.format(str(ds.a.voxel_eldim)),\n",
    "           'CPU              : {0}'.format(n_proc),\n",
    "           \n",
    "           '\\nChance Level     : {0:>5}'.format(round(chance_level, 5)*100),\n",
    "           'Accuracy (mean)  : {0:>5}'.format(round(mean_accuracy, 5)*100),\n",
    "           'Accuracy (std)   : {0:>5}'.format(round(std_accuracy, 5)*100),\n",
    "           \n",
    "           'above 2STD in %  : {0:>7}%'.format(round(percent_above_average(2), 3)),\n",
    "           'above 3STD in %  : {0:>7}%'.format(round(percent_above_average(3), 3)),\n",
    "           'above 2STD in v  : {0:>7}'.format(spheres_above_average(2)),\n",
    "           'above 3STD in v  : {0:>7}'.format(spheres_above_average(3)),\n",
    "           \n",
    "           '\\nDataset Summary:',\n",
    "           '****************',\n",
    "           '%s' % ds.summary(),\n",
    "           '%s' % ds.sa,\n",
    "           '\\n%s' % ds.summary]\n",
    "\n",
    "    # Write information to text file\n",
    "    results_file = opj(result_path, ds_name.replace('.hdf5', '_%s.rst' % clf_type))\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.writelines('\\n'.join(txt))\n",
    "        \n",
    "    # Return relevant information\n",
    "    results = [wall_time, chance_level, mean_accuracy, std_accuracy,\n",
    "               spheres_above_average(2), spheres_above_average(3)]\n",
    "    return results_file, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save searchlight result in NIfTI and show it in glassbrain plot\n",
    "\n",
    "This function stores the searchlight results in a NIfTI file and shows the output on a glassbrain plot, thresholded at a given value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_glass_brain\n",
    "from nilearn.image import new_img_like\n",
    "\n",
    "def plot_glassbrain(ds, sl_map, mask_img, results_file, threshold):\n",
    "\n",
    "    # Put searchlight output back into NIfTI space\n",
    "    sl_data = ds.mapper.reverse(sl_map.S)[0, ...]\n",
    "    sl_img = new_img_like(mask_img, sl_data)\n",
    "    sl_filename = results_file.replace('.rst', '.nii.gz')\n",
    "    sl_img.to_filename(opj(result_path, sl_filename))\n",
    "\n",
    "    # Plotting the searchlight results on the glass brain\n",
    "    title_txt = '{} - threshold = {}%'.format(clf_type, round(threshold * 100, 2))\n",
    "    plot_glass_brain(sl_img, black_bg=True, colorbar=True, title=title_txt,\n",
    "                     display_mode='lyrz', threshold=threshold, cmap='magma',\n",
    "                     output_file=results_file.replace('.rst', '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Searchlight Analysis\n",
    "\n",
    "Now that all important functions are specified, we can perform the searchlight analysis. As with a classical design, searchlight analysis apply a 1st-level and 2nd-level analysis. The **correct approach** to perform a 2nd-level analysis on searchlight results, i.e. accuracy maps, involves permutation (see [Stelzer et al. (2013)](https://www.sciencedirect.com/science/article/pii/S1053811912009810)). This notebook nonetheless, applies also a classical 2nd-level GLM, that can be used as an indication of results, but shouldn't be used for publications!\n",
    "\n",
    "In the rest of the notebook the following steps are conducted:\n",
    "\n",
    "1. **1st-level searchlight analysis** (with original labels) to acquire accuracy maps\n",
    "1. **classical 2nd-level GLM analysis**, based on those accuracy maps\n",
    "1. **1st-level searchlight analysis** with **permutated labels** (performed N-times)\n",
    "1. **2nd-level analysis** according **[Stelzer et al. (2013)](https://www.sciencedirect.com/science/article/pii/S1053811912009810)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st-level searchlight analysis (with original labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify order of parameter to iterate over\n",
    "if not session_list:\n",
    "    session_list = ['']\n",
    "    \n",
    "iteration_list = [[s, t, tf, sf, sess]\n",
    "                  for sess in session_list\n",
    "                  for tf in filters_temporal\n",
    "                  for sf in filters_spatial\n",
    "                  for t in tasks.keys()\n",
    "                  for s in subject_list]\n",
    "iteration_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over all parameters and run the searchlight analysis\n",
    "for subject_id, task_id, tFilter, sFilter, session_id in iteration_list:\n",
    "\n",
    "    # Create multivariate dataset\n",
    "    ds_path, mask_img, ds_name = create_dataset(\n",
    "        subject_id, session_id, task_id, tFilter, sFilter,\n",
    "        n_chunks, out_path, mask_file)\n",
    "\n",
    "    # Go through all targets\n",
    "    for targets in tasks[task_id]:\n",
    "\n",
    "        # Extract training and testing labels\n",
    "        train_labels = targets[0]\n",
    "        test_labels = targets[1]\n",
    "\n",
    "        # Specify results folder\n",
    "        result_path = opj(out_path, 'task-%s' % task_id, 'train-%s_test-%s' % (\n",
    "            '_'.join([str(s) for s in targets[0]]),\n",
    "            '_'.join([str(s) for s in targets[1]])))\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "\n",
    "        # Are the training and target labels the same?\n",
    "        cross_clf = targets[0] != targets[1]\n",
    "\n",
    "        # Prepare PyMVPA dataset for searchlight analysis\n",
    "        ds_sl = prepare_dataset(ds_path, train_labels, test_labels, cross_clf)\n",
    "\n",
    "        # Go through specified classifiers\n",
    "        for clf_name in clf_names:\n",
    "\n",
    "            # Perform searchlight analysis\n",
    "            sl, sl_map = run_searchlight(\n",
    "                ds_sl, clf_name, cross_clf, sphere_radius, sphere_steps, n_proc,\n",
    "                verbose=False)\n",
    "\n",
    "            # Create visual outputs and report file\n",
    "            clf_type = 'clf-%s_radius-%0d_steps-%03d' % (\n",
    "                clf_name, sphere_radius, sphere_steps)\n",
    "            results_file, results = create_output(\n",
    "                ds_sl, sl, sl_map, subject_id, clf_name, targets,\n",
    "                sphere_radius, sphere_steps, ds_name, clf_type,\n",
    "                result_path, n_proc)\n",
    "            wall_time, chance_level, mean_accuracy, std_accuracy, v2STD, v3STD = results\n",
    "            threshold = mean_accuracy + 2 * std_accuracy\n",
    "            plot_glassbrain(ds_sl, sl_map, mask_img, results_file, threshold)\n",
    "\n",
    "            # Print log stream to terminal\n",
    "            out_stream = '{} - Targets: {}'.format(\n",
    "                ds_name.replace('.hdf5', ''), '_'.join([str(t) for t in targets]))\n",
    "            out_stream += '\\n  {}  Radius: {:>5}   Steps: {:>4}   CLF: {}'.format(\n",
    "                wall_time, sphere_radius, sphere_steps, clf_name)\n",
    "            out_stream += '\\n            Chance: {:>5}%  Mean: {:>5}%  STD: {}%  +2STDv={} +3STDv={}'.format(\n",
    "                round(100*chance_level, 1), round(100*mean_accuracy, 1), round(100*std_accuracy, 1), v2STD, v3STD)\n",
    "            border_txt = '#' * 40\n",
    "            print('\\n'.join([border_txt, out_stream, border_txt]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical 2nd-level GLM analysis\n",
    "\n",
    "The application of a classical GLM approach to perform a 2nd-level analysis of searchlight results is not recommended. The correct way to perform a group analysis on searchlight accuracy maps is by applying permutation testing, as for example proposed by [Stelzer et al. (2013)](https://www.sciencedirect.com/science/article/pii/S1053811912009810)).\n",
    "\n",
    "Having said all this, the following code performs a classical 2nd-level GLM analysis and tests the acquired accuracy maps against chance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify order of parameter to iterate over\n",
    "if not session_list:\n",
    "    session_list = ['']\n",
    "    \n",
    "iteration_list = [[t, tf, sf, sess]\n",
    "                  for sess in session_list\n",
    "                  for tf in filters_temporal\n",
    "                  for sf in filters_spatial\n",
    "                  for t in tasks.keys()]\n",
    "iteration_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from nilearn.image import math_img\n",
    "from nilearn.plotting import plot_glass_brain\n",
    "from nistats.second_level_model import SecondLevelModel\n",
    "from nistats.thresholding import map_threshold\n",
    "\n",
    "# Iterate over all parameters and run the searchlight analysis\n",
    "for task_id, tFilter, sFilter, session_id in iteration_list:\n",
    "\n",
    "    # Create filter idx\n",
    "    tFilter_id = '%s.%s' % (tFilter[0], tFilter[1])\n",
    "    sFilter_id = '%s.%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "    # Go through all targets\n",
    "    for targets in tasks[task_id]:\n",
    "        \n",
    "        # Specify classification folder\n",
    "        clf_folder = 'train-%s_test-%s' % (\n",
    "            '_'.join([str(s) for s in targets[0]]),\n",
    "            '_'.join([str(s) for s in targets[1]]))\n",
    "\n",
    "        # Go through specified classifiers\n",
    "        for clf_name in clf_names:\n",
    "            \n",
    "            # Collect all the files\n",
    "            file_idx = 'sub-*_tFilter-%s_sFilter-%s_mask-%.03f' % (tFilter_id, sFilter_id, gm_mask_thr)\n",
    "            file_idx += '_clf-%s_radius-%0d_steps-%03d.nii.gz' % (clf_name, sphere_radius, sphere_steps)\n",
    "            sl_res = sorted(glob(opj(out_path, 'task-%s' % task_id, clf_folder, file_idx)))\n",
    "            \n",
    "            # Path to output folder\n",
    "            result_path = opj(out_path, 'task-%s' % task_id, 'group_glm', clf_folder)\n",
    "            if not os.path.exists(result_path):\n",
    "                os.makedirs(result_path)\n",
    "                \n",
    "            # Center searchlight accuracy maps around chance level\n",
    "            chance_level = 1.0 / len(targets[0])\n",
    "            sl_imgs = [math_img('img - %s' % chance_level, img=sl) for sl in sl_res]\n",
    "            \n",
    "            # Specify 2nd-level GLM design matrix\n",
    "            design_matrix = pd.DataFrame([1] * len(sl_res), columns=['intercept'])\n",
    "\n",
    "            # Estimate 2nd-level Model\n",
    "            second_level_model = SecondLevelModel()\n",
    "            second_level_model = second_level_model.fit(\n",
    "                sl_imgs, design_matrix=design_matrix)\n",
    "\n",
    "            # Compute z-score\n",
    "            z_map = second_level_model.compute_contrast(output_type='z_score')\n",
    "            out_filename = opj(result_path, file_idx.replace('sub-*', 'z-map'))\n",
    "            z_map.to_filename(out_filename)\n",
    "\n",
    "            # Threshold output with different approaches and save figure\n",
    "            for l, h in [(.001, 'fpr'), (.05, 'fdr'), (.05, 'bonferroni')]:\n",
    "            \n",
    "                thr_name = out_filename.replace('z-map', 'thr_%s_%.03f' % (h, l))\n",
    "                thr_map, thr = map_threshold(z_map, level=l, height_control=h)\n",
    "                thr_map.to_filename(thr_name)\n",
    "                \n",
    "                plot_glass_brain(\n",
    "                    z_map, threshold=thr, colorbar=True, black_bg=True, plot_abs=False,\n",
    "                    display_mode='lyrz',  output_file=thr_name.replace('.nii.gz', '.png'),\n",
    "                    title='%s: %s - Threshold = %.03f' % (h, l, thr))\n",
    "            \n",
    "            log_txt = file_idx.replace('sub-*_', '').replace('.nii.gz', '').split('_')\n",
    "            print('{:<20} {:<16} {:<12} {:<18} {:<10} {:<12} finished.'.format(*log_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st-level searchlight analysis (with label permutation)\n",
    "\n",
    "Depending on the number of permutations per subject, this step will take a long time to compute. It is recommended to take the following code and run it on a cluster server, where you can profit from real parallelization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify order of parameter to iterate over\n",
    "if not session_list:\n",
    "    session_list = ['']\n",
    "    \n",
    "iteration_list = [[s, t, tf, sf, sess]\n",
    "                  for sess in session_list\n",
    "                  for tf in filters_temporal\n",
    "                  for sf in filters_spatial\n",
    "                  for t in tasks.keys()\n",
    "                  for s in subject_list]\n",
    "iteration_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Iterate over all parameters and run the searchlight analysis\n",
    "for subject_id, task_id, tFilter, sFilter, session_id in iteration_list:\n",
    "\n",
    "    # Create multivariate dataset\n",
    "    ds_path, mask_img, ds_name = create_dataset(\n",
    "        subject_id, session_id, task_id, tFilter, sFilter,\n",
    "        n_chunks, out_path, mask_file)\n",
    "\n",
    "    # Create filter idx\n",
    "    tFilter_id = '%s.%s' % (tFilter[0], tFilter[1])\n",
    "    sFilter_id = '%s.%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "    # Go through all targets\n",
    "    for targets in tasks[task_id]:\n",
    "\n",
    "        # Extract training and testing labels\n",
    "        train_labels = targets[0]\n",
    "        test_labels = targets[1]\n",
    "\n",
    "        # Specify results folder\n",
    "        result_path = opj(out_path, 'task-%s' % task_id, 'train-%s_test-%s' % (\n",
    "            '_'.join([str(s) for s in targets[0]]),\n",
    "            '_'.join([str(s) for s in targets[1]])), 'permutations')\n",
    "        if not os.path.exists(result_path):\n",
    "            os.makedirs(result_path)\n",
    "\n",
    "        # Are the training and target labels the same?\n",
    "        cross_clf = targets[0] != targets[1]\n",
    "\n",
    "        # Prepare PyMVPA dataset for searchlight analysis\n",
    "        ds_sl = prepare_dataset(ds_path, train_labels, test_labels, cross_clf)\n",
    "\n",
    "        # Go through specified classifiers\n",
    "        for clf_name in clf_names:\n",
    "\n",
    "            # Go through all permutations, but verify that none occures double\n",
    "            perm = AttributePermutator('targets')\n",
    "\n",
    "            used_combinations = []\n",
    "            used_combinations.append(''.join(ds_sl.targets))\n",
    "\n",
    "            p = 0\n",
    "            while p < n_perm - 1:\n",
    "\n",
    "                # Permutate dataset\n",
    "                ds_perm = perm(ds_sl)\n",
    "\n",
    "                # Catch searchlight analysis that break\n",
    "                try:\n",
    "                \n",
    "                    # Verify that permutation is unique\n",
    "                    counter = 0\n",
    "                    while (''.join(ds_perm.targets) in used_combinations) and counter < n_perm:\n",
    "                        ds_perm = perm(ds_sl)\n",
    "                        counter += 1\n",
    "\n",
    "                    # Perform searchlight if permutation limit is not reached yet\n",
    "                    if counter != n_perm:\n",
    "\n",
    "                        used_combinations.append(''.join(ds_perm.targets))\n",
    "\n",
    "                        sl, sl_map = run_searchlight(\n",
    "                            ds_perm, clf_name, cross_clf, sphere_radius, sphere_steps, n_proc,\n",
    "                            verbose=False)\n",
    "\n",
    "                        # Save result in a numpy object\n",
    "                        out_name = 'sub-%s_tFilter-%s_sFilter-%s_mask-%.03f' % (\n",
    "                            subject_id, tFilter_id, sFilter_id, gm_mask_thr)\n",
    "                        out_name += '_clf-%s_radius-%0d_steps-%03d' % (\n",
    "                            clf_name, sphere_radius, sphere_steps)\n",
    "                        out_name += '_nperm-%03d.npy' % (p + 1)\n",
    "\n",
    "                        if session_id:\n",
    "                            out_name.replace('_tFilter', 'ses-%s_tFilter' % session_id)\n",
    "\n",
    "                        np.save(opj(result_path, out_name), sl_map.samples)\n",
    "\n",
    "                        # Print log stream to terminal\n",
    "                        wall_time = time.strftime('%H:%M:%S', time.gmtime(round(sl.ca.calling_time)))\n",
    "                        print('{} {}'.format(wall_time, basename(out_name)[:-4]))\n",
    "\n",
    "                    p += 1\n",
    "                \n",
    "                except FailedToTrainError as e:\n",
    "                    print 'Searchlight was restarted because of error: %s' % e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd-level analysis according to [Stelzer et al. (2013)](https://www.sciencedirect.com/science/article/pii/S1053811912009810)\n",
    "\n",
    "This next step is also time consuming, but luckily is much lower than the previous permutation process. It should be finished within a few hours.\n",
    "\n",
    "First, we need to collect all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify order of parameter to iterate over\n",
    "if not session_list:\n",
    "    session_list = ['']\n",
    "    \n",
    "iteration_list = [[t, tf, sf, sess]\n",
    "                  for sess in session_list\n",
    "                  for tf in filters_temporal\n",
    "                  for sf in filters_spatial\n",
    "                  for t in tasks.keys()]\n",
    "iteration_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Iterate over all parameters and run the searchlight analysis\n",
    "for task_id, tFilter, sFilter, session_id in iteration_list:\n",
    "\n",
    "    # Create filter idx\n",
    "    tFilter_id = '%s.%s' % (tFilter[0], tFilter[1])\n",
    "    sFilter_id = '%s.%s' % (sFilter[0], sFilter[1])\n",
    "\n",
    "    # Go through all targets\n",
    "    for targets in tasks[task_id]:\n",
    "        \n",
    "        # Specify classification folder\n",
    "        clf_folder = 'train-%s_test-%s' % (\n",
    "            '_'.join([str(s) for s in targets[0]]),\n",
    "            '_'.join([str(s) for s in targets[1]]))\n",
    "\n",
    "        # Go through specified classifiers\n",
    "        for clf_name in clf_names:\n",
    "\n",
    "            # Path to output folder\n",
    "            result_path = opj(out_path, 'task-%s' % task_id, 'group_stelzer', clf_folder)\n",
    "            if not os.path.exists(result_path):\n",
    "                os.makedirs(result_path)\n",
    "                \n",
    "            # Collect all files\n",
    "            file_idx = 'sub-*_tFilter-%s_sFilter-%s_mask-%.03f' % (\n",
    "                tFilter_id, sFilter_id, gm_mask_thr)\n",
    "            file_idx += '_clf-%s_radius-%0d_steps-%03d.nii.gz' % (\n",
    "                clf_name, sphere_radius, sphere_steps)\n",
    "            path_orig = sorted(glob(opj(\n",
    "                out_path, 'task-%s' % task_id, clf_folder, file_idx)))\n",
    "            path_perm = sorted(glob(opj(\n",
    "                out_path, 'task-%s' % task_id, clf_folder, 'permutations',\n",
    "                file_idx.replace('.nii.gz', '_nperm-*.npy'))))\n",
    "            \n",
    "            # Extract data from all fils\n",
    "            mask_id = 'sub-%s_tFilter-%s_sFilter-%s_mask-%.03f.hdf5' % (\n",
    "                subject_list[0], tFilter_id, sFilter_id, gm_mask_thr)\n",
    "            if session_id:\n",
    "                ds_name.replace('_tFilter', 'ses-%s_tFilter' % session_id)\n",
    "            mask_mapper = h5load(opj(out_path, 'task-%s' % task_id, mask_id))\n",
    "            acc_orig = np.vstack([mask_mapper.a.mapper[:2].forward1(\n",
    "                nb.load(orig).get_data()) for orig in path_orig])\n",
    "            acc_perm = np.vstack([np.load(perm).ravel() for perm in path_perm])\n",
    "            \n",
    "            # Create PyMVPA datasets for group analysis\n",
    "            orig_ds = Dataset(acc_orig,\n",
    "                              sa=dict(subj=subject_list),\n",
    "                              fa=mask_mapper.fa, a=mask_mapper.a)\n",
    "            perm_ds = Dataset(np.vstack((acc_orig, acc_perm)),\n",
    "                              sa=dict(subj=np.hstack(\n",
    "                                  (subject_list,\n",
    "                                   np.repeat(subject_list, n_perm - 1)))),\n",
    "                              fa=mask_mapper.fa, a=mask_mapper.a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.datasets.mri import map2nifti\n",
    "from mvpa2.algorithms.group_clusterthr import GroupClusterThreshold\n",
    "\n",
    "# Debugger on or off\n",
    "if __debug__:\n",
    "    from mvpa2.base import debug\n",
    "    debug.active = ['GCTHR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.001\n",
    "thr = GroupClusterThreshold(n_bootstrap=1000,\n",
    "                            feature_thresh_prob=threshold,\n",
    "                            chunk_attr='subj',\n",
    "                            fwe_rate=0.05,\n",
    "                            multicomp_correction='fdr_bh',\n",
    "                            n_proc=n_proc)\n",
    "# bonferroni, sidak, holm-sidak, holm, simes-hochberg, hommel, fdr_bh, fdr_by, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "thr.train(perm_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = thr(orig_ds)\n",
    "h5save(opj(experiment_dir, 'medusa_new', 'results_group_%s' % identifier,\n",
    "           'grpavg_stats_%s_thr%s.hdf5' % (identifier, threshold)),\n",
    "       res, compression=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector with average accuracy\n",
    "nb.save(map2nifti(res, res.samples),\n",
    "        opj(experiment_dir, 'medusa_new', 'results_group_%s' % identifier,\n",
    "            'grpavg_avg_acc_%s_thr%s.nii.gz' % (identifier, threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector with feature-wise cluster-forming thresholds.\n",
    "nb.save(map2nifti(res, res.fa.featurewise_thresh),\n",
    "        opj(experiment_dir, 'medusa_new', 'results_group_%s' % identifier,\n",
    "            'grpavg_featurewise_thresh_%s_thr%s.nii.gz' % (identifier,\n",
    "                                                           threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector with labels for clusters after thresholding. Cluster values are sorted\n",
    "# by cluster size. The largest cluster is always labeled with ``1``.\n",
    "nb.save(map2nifti(res, res.fa.clusters_featurewise_thresh),\n",
    "        opj(experiment_dir, 'medusa_new', 'results_group_%s' % identifier,\n",
    "            'grpavg_clusters_featurewise_thresh_%s_thr%s.nii.gz' % (identifier,\n",
    "                                                                    threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Vector as above but with accuracies\n",
    "nb.save(map2nifti(res, np.array(res.fa.clusters_featurewise_thresh != 0).astype('int') * res.samples),\n",
    "        opj(experiment_dir, 'medusa_new', 'results_group_%s' % identifier,\n",
    "            'grpavg_clusters_featurewise_thresh_%s_thr%s_acc.nii.gz' % (identifier,\n",
    "                                                                        threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector with labels for super-threshold clusters after FWE correction. The\n",
    "# attribute is derived from ``fa.clusters_featurewise_thresh`` by removing all\n",
    "# clusters that do not pass the threshold when controlling for FWE.\n",
    "if hasattr(res.fa, 'clusters_fwe_thresh'):\n",
    "    nb.save(map2nifti(res, res.fa.clusters_fwe_thresh),\n",
    "            opj(experiment_dir, 'medusa_new', 'results_group_%s' % identifier,\n",
    "                'grpavg_clusters_fwe_thresh_%s_thr%s.nii.gz' % (identifier,\n",
    "                                                                threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Vector as above but with accuracies\n",
    "nb.save(map2nifti(res, np.array(res.fa.clusters_fwe_thresh != 0).astype('int') * res.samples),\n",
    "        opj(experiment_dir, 'medusa_new', 'results_group_%s' % identifier,\n",
    "            'grpavg_clusters_fwe_thresh_%s_thr%s_acc.nii.gz' % (identifier,\n",
    "                                                                threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the record array with information on all detected clusters. The array\n",
    "# contains the fields size (number of features comprising the cluster), and\n",
    "# prob_raw (probability of observing the cluster of a this size or larger under\n",
    "# the NULL hypothesis). If correction for multiple comparisons is enabled an\n",
    "# additional field prob_corrected (probability after correction) is added.\n",
    "# The array also contains the fields ``max`` (feature coordinate of the maximum\n",
    "# score within the cluster, and ``center_of_mass`` (coordinate of the center of\n",
    "# mass; weighted by the feature values within the cluster.\n",
    "with open(opj(experiment_dir, 'medusa_new', 'results_group_%s' % identifier,\n",
    "              'clusterstats_%s_thr%s.csv' % (identifier, threshold)),\n",
    "          'w') as sFile:\n",
    "\n",
    "    header = [e[0] for e in res.a.clusterstats.dtype.descr] + [e[0]\n",
    "                                                               for e in res.a.clusterlocations.dtype.descr]\n",
    "\n",
    "    content = [','.join([str(e) for e in c] +\n",
    "                        ['(%s)' % ' '.join([str(l[0]) for l in res.a.clusterlocations[i]]),\n",
    "                         '(%s)' % ' '.join([str(l[1]) for l in res.a.clusterlocations[i]])])\n",
    "               for i, c in enumerate(res.a.clusterstats)]\n",
    "\n",
    "    sFile.write(','.join(header) + '\\n')\n",
    "    sFile.write('\\n'.join(content))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:mvpa]",
   "language": "python",
   "name": "conda-env-mvpa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
